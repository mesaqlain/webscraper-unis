{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1542ff4b",
   "metadata": {},
   "source": [
    "# WEB SCRAPER FOR DYNAMIC WEBSITES\n",
    "Written by Murshid Saqlain  \n",
    "Last updated: July 13, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e0c3f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#intro)\n",
    "2. [Prerequisites](#prereq)\n",
    "3. [Pre-scraping Tasks](#prescrape)\n",
    "4. [BigFuture (BF) Website](#bf)\n",
    "    * [Scrape Single BF URL](#bf-single)\n",
    "    * [Scrape Multiple BF URL's](#bf-multi)\n",
    "    * [Create BF Dataframe](#bf-pandas)\n",
    "5. [CollegeData (CD) Website](#cd)\n",
    "    * [Scrape Single CD URL](#cd-single)\n",
    "    * [Scrape Multiple CD URL's](#cd-multi)\n",
    "    * [Create CD Dataframe](#cd-pandas)\n",
    "6. [Merge Dataframes](#merge)\n",
    "7. [Conclusion](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2d7e8",
   "metadata": {
    "tags": [
     "intro"
    ]
   },
   "source": [
    "## Introduction<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192120b",
   "metadata": {},
   "source": [
    "### **What is Web Scraping?**  \n",
    "Web scraping is an efficient way to collect data from websites when an API (Application Programming Interface) is not readily available. Want to collect data on stock prices? Football match results? Job posting information? Web scraping can help streamline the process and let you avoid having to go through each url separately and manually.\n",
    "\n",
    "### **What are Dynamic Websites?**  \n",
    "While static websites can be easily parsed with `Beautiful Soup` and `Requests` in Python, dynamic websites pose a bit of trouble. Dynamic websites are a bit trickier to scrape because all the information isn't sent client side directly in its static HTML. It displays content based on user's choices usually (such as drop down menus, filters, etc.)  \n",
    "\n",
    "One quick way to identify whether a website is dynamic or not is to *disable JavaScript*. If JS is disabled, the dynamic content from the site will disappear!\n",
    "\n",
    "### **How to disable JavaScript?**  \n",
    "1. Load your desired webpage.\n",
    "2. Open Developer Tools in your browser by right clicking anywhere and selecting Inspect.\n",
    "3. Open comand palette by pressing CTRL + Shift + P\n",
    "4. Type JavaScript in the search bar that shows up\n",
    "5. Click on Disable JavaScript [Debugger]\n",
    "6. Refresh the webapge.\n",
    "\n",
    "If some of the content has disappeared from the webpage, it means the website is dynamic!\n",
    "\n",
    "### **What are we scraping today?**  \n",
    "We're going to be gathering data on certain universities from two different websites:\n",
    "1. **BigFuture.org**\n",
    "2. **CollegeData.com**  \n",
    "\n",
    "Some of the information is not present readily in BigFuture.org, so we're also going to be parsing data from CollegeData.com. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f2d22",
   "metadata": {
    "tags": [
     "prereq"
    ]
   },
   "source": [
    "## Prerequisites<a id='prereq'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d019f28",
   "metadata": {},
   "source": [
    "### **How do we scrape Dynamic Wesbites?**  \n",
    "We'll use the powerful Python package **Selenium**, **Google Chome**, and **ChromeDriver** to scrape dynamic websites. Of course, you can use other browsers as well, such as FireFox and you'd need the apppropriate driver, but for my example, I'm sticking to Chrome.  \n",
    "\n",
    "**Selenium** is a python package that executes the JS so that we can scrape information from the website as if we are viewing the website from our browser. According to the official **ChromeDriver** website, \"WebDriver is an open source tool for automated testing of webapps across many browsers. It provides capabilities for navigating to web pages, user input, JavaScript execution, and more.\" Using these two together, we are able to navigate through the dynamic websites and execute the JS in a way that we are able to parse that information easily from our end.\n",
    "\n",
    "### **How to install Selenium?**  \n",
    "Open your terminal (I'm using miniconda), load up your virtual environment (I prefer making different environments for different projects), and type `pip install selenium`. Before we start coding, we will import this package.\n",
    "\n",
    "### **How to install ChromeDriver?**  \n",
    "You can find the binaries here: https://chromedriver.chromium.org/downloads. Ensure that the version of ChromeDriver matches the version of your Google Chrome. You can check the version of your Google Chrome by clicking on the three dots icon on the top right of your browser, on Help, and on About Google Chrome. This will open a new window that will display the version of your Google Chrome. From the chromedriver website, get the same version. \n",
    "\n",
    "You will need to extract the zip file (assuming you are on windows like me) and then move the contents to the location where your notebook is located (if you're following my example).\n",
    "\n",
    "### Any other requirements?\n",
    "We'll use `regex` for extracting specific information from strings, `pandas` for working with dataframes, and `time` to add small delays within our scraping scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935d84f",
   "metadata": {
    "tags": [
     "prescrape"
    ]
   },
   "source": [
    "## Pre-Scraping Tasks <a id='prescrape'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f424c",
   "metadata": {},
   "source": [
    "### **Web Page URLS**  <a id='webpages'></a>\n",
    "To scrape information from a website, we need an URL of that webpage. However, this notebook **does not** fetch an exhaustive list of all the universities available in these websites. That is beyond the scope of this particular notebook (maybe I'll make a separate notebook just for that!). So, we're going to be providing a list of URLS for specific universities. \n",
    "\n",
    "On a sidenote, the URL creation is a fairly easy process and can be done using python as long as we have a list of the university names. This is because we notice that the links for specific universities have a common pattern (Thanks to Aurpa for pointing this out). \n",
    "\n",
    "For BigFuture.org, they all have a prefix of https://bigfuture.collegeboard.org/colleges/ followed by the college name with spaces replaced with hyphens. Some examples:\n",
    "* Allegheny College: https://bigfuture.collegeboard.org/colleges/allegheny-college\n",
    "* Beloit College: https://bigfuture.collegeboard.org/colleges/beloit-college\n",
    "\n",
    "Similarly, for CollegeData.com, all the url's have a prefix of https://waf.collegedata.com/college-search/. Some examples:\n",
    "* Allegheny College: https://waf.collegedata.com/college-search/allegheny-college\n",
    "* Beloit College: https://waf.collegedata.com/college-search/Beloit-College\n",
    "\n",
    "With this information we can quickly populate our list of url's as long as we have a list of university names. We use `.lower()` to force lowercase on the names, `.strip()` to remove any leading and trailing spaces, and `.replace(' ', '-')` to replace spaces with hyphens.\n",
    "\n",
    "First we create an empty list called url_bf for BigFuture URLS (and url_cd for CollegeData). We use loop to iterate over our university names and for each name, we'll add the appropriate prefix to the lowercased, stripped, and hyphenated version of the name. This url is then appended to the url_bf list as we loop through the entire list.\n",
    "\n",
    "Note: Some url's had to be manually changed because they didn't follow the above pattern, so they had to be fixed manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de20bd6",
   "metadata": {},
   "source": [
    "**Example Code** is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3a3ddde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://bigfuture.collegeboard.org/colleges/university-of-minnesota/', 'https://bigfuture.collegeboard.org/colleges/new-york-university/', 'https://bigfuture.collegeboard.org/colleges/amherst-college/']\n",
      "['https://waf.collegedata.com/college-search/university-of-minnesota/', 'https://waf.collegedata.com/college-search/new-york-university/', 'https://waf.collegedata.com/college-search/amherst-college/']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of URLS from university names\n",
    "\n",
    "# Univeristy names:\n",
    "uni_names = ['University of Minnesota', 'New York University', 'Amherst College']\n",
    "             \n",
    "# Prefix for BigFuture.org and CollegeData.com links     \n",
    "prefix_bf = \"https://bigfuture.collegeboard.org/colleges/\"\n",
    "prefix_cd = \"https://waf.collegedata.com/college-search/\"\n",
    "\n",
    "# Craete empty lists for our URLs:\n",
    "url_bf = [] #BigFuture\n",
    "url_cd = [] #CollegeData\n",
    "\n",
    "# URLS for the site\n",
    "for name in uni_names:\n",
    "    url_bf.append(prefix_bf + name.lower().strip().replace(' ', '-') + '/') #BigFuture\n",
    "    url_cd.append(prefix_cd + name.lower().strip().replace(' ', '-') + '/') #CollegeData\n",
    "\n",
    "# Print our URL lists:    \n",
    "print(url_bf)\n",
    "print(url_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1cead",
   "metadata": {
    "tags": [
     "bf"
    ]
   },
   "source": [
    "## EXPLORE BIGFUTURE.ORG:<a id='bf'></a>\n",
    "\n",
    "Before we can do any kind of scraping, it's good practice to get a good idea about the website, the layout, where information is located, which information we're insterested in, etc. Let's take a look at BigFuture site as an example. I'll load up an example link: https://bigfuture.collegeboard.org/colleges/allegheny-college with Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "3d32c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "#The chromedriver_win32 folder is in the same directory where this notebook is located\n",
    "DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "\n",
    "# Initiate the webdriver\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "# This will open a new chrome window\n",
    "driver.get(\"https://bigfuture.collegeboard.org/colleges/allegheny-college/\")\n",
    "\n",
    "# Uncomment following to end session and close window\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943d0e1",
   "metadata": {},
   "source": [
    "Notice that at the top, you'll see a message: `Chrome is being controlled by automated test software`, this is because we are using Selenium and WebDriver to open this window. I should point out here that any time you are done exploring or scraping a page, you want to execute `driver.quit()` to close the window and end the session. \n",
    "\n",
    "Another thing you'll notice is the \"Accept All Cookies\" popup. This is important, because we're going to have to deal with this if we are to scrape multiple urls in one go. Each time we open a new page, we're going to have to click on that X button or Accept all Cookies to get rid of that popup, otherwise, it'll cause issues when collecting our data later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00cfdc",
   "metadata": {},
   "source": [
    "**Headless Mode**  \n",
    "It's great that we can open a new chromw window to explore the website and all the elements inside it, but when we are scraping many, many url's, we don't really want to see a new window open every time. There's a solution for that as well. If we set Options.headless = True, and then we use  these options when initiating our webdriver, we'll essentially be performing all our scraping tasks without opening the window!\n",
    "\n",
    "`from selenium.webdriver.chrome.options import Options`  \n",
    "`options = Options()`  \n",
    "`options.headless = True`  \n",
    "`driver = webdriver.Chrome(options = options, executable_path=DRIVER_PATH)`\n",
    "\n",
    "You can also set the window size in options (check Selenium documentation), but I do not use it here. For now, let's keep exploring the site without using headless = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3c491ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Uncomment this section to execute our webdriver without opening a new window!\n",
    "# # Imports\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "# options = Options()\n",
    "# options.headless = True\n",
    "# driver = webdriver.Chrome(options = options, executable_path=DRIVER_PATH)\n",
    "# driver.get(\"https://bigfuture.collegeboard.org/colleges/allegheny-college/\")\n",
    "\n",
    "# # Uncomment following to end session and close window\n",
    "# #driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04710a7",
   "metadata": {},
   "source": [
    "**Screenshot of Allegheny College info on BigFuture.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed881fa",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/bigfuture_allegheny_home.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb810f",
   "metadata": {},
   "source": [
    "The base link loads the `About` page by default. We can click on `Admissions`, `Academics`, `Costs`, and `Campus Life` tabs to get more specific information in those categories.   \n",
    "\n",
    "Before we start gathering some data, let's deal with that annoying popup! Right click on the button 'Accept All Cookies' and click **Inspect**. This will open the developer tool. If it doesn't automatically take you to the element where this accept cookies button is located, right click on that button one more time and click inspect again. This should take you to the appropriate button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ebe66",
   "metadata": {},
   "source": [
    "<img src =\"./images/cookies_button.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30dc925",
   "metadata": {},
   "source": [
    "Luckily, this button has a specific button id called **onetrust-accept-btn-handler**, which we can use to our advantage. We also have to consider that this popup may not always show up, so we need a way to handle exceptions if that's the case. Which is why we load `NoSuchElementException`.  \n",
    "\n",
    "`NoSuchElementException` is used more frequently later in the notebook as well, because we'll notce that some universities may not have all the information. If certain classes, id, etc. are missing, the script will crash! So we need a way to handle these exceptions.\n",
    "\n",
    "Also, notice that we use the `By` function here, but we could have also written it with `find_element_by_id` to same effect.\n",
    "\n",
    "Load up the webpage and run the following script. You'll notice that Selenium will automatically click on Accept Cookies for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "cfa4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Code to click on Accept All Cookies, or do nothing if it doesn't exist!\n",
    "try:\n",
    "    cookies_button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "    driver.execute_script(\"arguments[0].click();\", cookies_button)\n",
    "except NoSuchElementException:\n",
    "    pass  # Skip clicking if the element is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17509d42",
   "metadata": {},
   "source": [
    "### **Data to Collect**\n",
    "\n",
    "Okay, now we're ready to find some data! You can play around the websites and explore some more, but after going through the different tabs, I decided on collecting these information:\n",
    "\n",
    "**About Page**:\n",
    "* University Name\n",
    "* City\n",
    "* State\n",
    "* Number of Years (4 / 2 / etc.)\n",
    "* Control (Private / Public)\n",
    "* Size (Small / Medium / Grande? / Venti?)\n",
    "* Setting (Urban / Suburban / Rural)\n",
    "* SAT Range (denoted by two numbers such as 1200-1400)\n",
    "* Whether it's a liberal arts college or not\n",
    "\n",
    "**Admissions Tab**:  \n",
    "* Acceptance Rate\n",
    "* Selectivity / Difficulty\n",
    "* Application Deadline for Regular Decision\n",
    "* Application Deadline for Early Action\n",
    "* Application Deadline for Early Decision\n",
    "\n",
    "**Costs Tab**:  \n",
    "* Net Cost\n",
    "* Tuition\n",
    "* Housing\n",
    "* Books\n",
    "* Personal Expenses\n",
    "* Avg Percent of Need Met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848fb56",
   "metadata": {},
   "source": [
    "## Let's Start Scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2ae76",
   "metadata": {},
   "source": [
    "### Inspecting Elements\n",
    "\n",
    "**University Name**  \n",
    "Now comes the most fun (and sometimes most frustrating) part of webscraping. We'll find and inspect the elements we are interested in and hope that there is a specific id/class name/selector for that element so that we can scrape that info consistently for all our urls. A  good practice is to load up multiple url's and make sure that the id/class/selector is the same in each url for the same element.\n",
    "\n",
    "For example, we are interested in the University Name that shows up. In this case, it's Allegheny College. We do the same as before: highlight `Allegheny College`, right click, and Inspect. This opens up Dev Tools. Repeat the Inspect again to highlight the location where the element is located in the JS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e31bc",
   "metadata": {},
   "source": [
    "<img src = './images/allegheny_college.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46f866",
   "metadata": {},
   "source": [
    "If you right click on that element in the Dev Tools and copy element, you will get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b64c2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h1 id=\"csp-banner-section-school-name-label\" class=\"_3Bdh-pnoD5EZYZQbT89tl\">Allegheny College</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a0f80",
   "metadata": {},
   "source": [
    "At this point you should perform some checks to make sure that this element will always return the university name, regardless of the URL. \n",
    "\n",
    "1. Copy the id part `csp-banner-section-school-name-label`. \n",
    "2. Hit CTRL + F to open search filter in dev tools. \n",
    "3. Paste the id name `csp-banner-section-school-name-label` in the search bar.\n",
    "\n",
    "You should see that there's only 1 match, which means that this id is unique. Now repeat the above for a different url and make sure that id shows you the university name. This is good practice to do before scraping multiple web pages at once. You want to make sure the website is consistent across different url's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ac345",
   "metadata": {},
   "source": [
    "<img src = './images/uni_name.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436ae16",
   "metadata": {},
   "source": [
    "Now that we've confirmed the specific id for university name, we're ready to fetch that information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f06eaab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University Name: Allegheny College\n"
     ]
    }
   ],
   "source": [
    "#UNI NAME\n",
    "uni_name = driver.find_element_by_id(\"csp-banner-section-school-name-label\").text\n",
    "print(f'University Name: {uni_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c55b34",
   "metadata": {},
   "source": [
    "Done! We have scraped our first piece of information and stored it in a variable called uni_name. Notice that we used `find_element_by_id()` function. We will use different functions later depending on what we need! Oh, and don't forget to use the `.text` at the end to only get the text inside that wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0b4f1",
   "metadata": {},
   "source": [
    "**City and State**  \n",
    "\n",
    "Next, we want the city and state information for the university. It's written right below the University Name which we fetched in the cell above. We repeat the same process as above by inspecting the element and checking what it shows for the selector/locator now. \n",
    "\n",
    "<img src = \"./images/city_state.jpg\">\n",
    "\n",
    "The id appears to be `csp-banner-section-school-location-label`. Again, I check the other url's to make sure this is always the case. Now we're ready to get this information as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d8ae1ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meadville, PA\n"
     ]
    }
   ],
   "source": [
    "# STATE AND CITY\n",
    "location = driver.find_element_by_id(\"csp-banner-section-school-location-label\").text\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97b054",
   "metadata": {},
   "source": [
    "Great! We have city and state, but there's a little problem. This is one string; ideally we want two different variables - one for city and one for state. So we do a little string manipulation to get what we need.  \n",
    "\n",
    "`.split(',')` will split the string into two, separating it at the comma as we specified. We'll make use of `.strip()` again to get rid of leading and trailing spaces for the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e051cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Meadville\n",
      "State: PA\n"
     ]
    }
   ],
   "source": [
    "# Split location string into two parts\n",
    "parts = location.split(',')\n",
    "# Removing any leading or trailing whitespace\n",
    "city_name = parts[0].strip()\n",
    "print(f'City: {city_name}')\n",
    "# Extracting the state, removing any leading or trailing whitespace\n",
    "state_code = parts[1].strip()\n",
    "print(f'State: {state_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25593a",
   "metadata": {},
   "source": [
    "Done! Now we've scraped the City and State information as well. We're making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf422d35",
   "metadata": {},
   "source": [
    "**Number of Years**\n",
    "\n",
    "The next information we want is how many years of study is required for that university. Notice that other critical information we want are in the same region (control, size, setting)!\n",
    "\n",
    "<img src = \"./images/overview.jpg\">\n",
    "\n",
    "We repeat the same process as above, and come up with the following:\n",
    "<img src = \"./images/years.jpg\">\n",
    "\n",
    "You should spot the difference from above. There isn't a specific `id = \"\"` like before. Instead it's wrapped inside the li header with `data-testid=\"csp-overview-school-type-by-years\"`. You can get this by Right Clicking on the element and Copying Element.\n",
    "\n",
    "We can't exactly use `find_element_by_id()` here. Instead, we'll use `find_element_by_css_selector()`. As always make sure that it is unique by copying that selector and making sure there's only one matching element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "48d10e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-year\n"
     ]
    }
   ],
   "source": [
    "# NO. OF YEARS\n",
    "no_of_years = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-years\"]').text\n",
    "print(no_of_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4b0d3",
   "metadata": {},
   "source": [
    "Great! We have the info, but what if I don't want the \"-year\" part of that string? Well let's use some regex to get only the number. (If we know  that it's ALWAYS going to be in that format, we could have also easily just used `no_of_years[0]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8c34918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "no_of_years = re.search(r'\\d+', no_of_years).group() # the d+ finds numbers\n",
    "print(f'No of years required: {no_of_years}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3253f5",
   "metadata": {},
   "source": [
    "**Control, Size, Setting**  \n",
    "Similarly, we'll get the other information from that line - Control, Size, Setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "5a908242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni Control: Private\n",
      "Uni Size: Small\n",
      "Uni Setting: Suburban\n"
     ]
    }
   ],
   "source": [
    "# UNI CONTROL\n",
    "uni_ctrl = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-designation\"]').text\n",
    "print(f'Uni Control: {uni_ctrl}')\n",
    "\n",
    "# UNI SIZE \n",
    "uni_size = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-size\"]').text\n",
    "print(f'Uni Size: {uni_size}')\n",
    "\n",
    "# UNI SETTING\n",
    "uni_setting = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-setting\"]').text\n",
    "print(f'Uni Setting: {uni_setting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cf377",
   "metadata": {},
   "source": [
    "Done! Next we move on to getting the SAT score range:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241063e3",
   "metadata": {},
   "source": [
    "**SAT SCORE**\n",
    "\n",
    "We repeat the same process as above. Highlight the SAT score ranges, and find the respective element. We can find_by_css_selector yet again. Copying element gives us the following:\n",
    "\n",
    "\n",
    "`<div class=\"csp-overview-info-text\" data-testid=\"csp-overview-sat-range\"><strong class=\"cb-roboto-medium\">1110–1370</strong></div>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "29e57b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110–1370\n"
     ]
    }
   ],
   "source": [
    "# SAT SCORES\n",
    "sat_range = driver.find_element_by_css_selector('.csp-overview-info-text[data-testid=\"csp-overview-sat-range\"]').text\n",
    "print(sat_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb18e3d",
   "metadata": {},
   "source": [
    "Okay, good, but I want to separate those two numbers in different columns, so one will be SAT Min and the other will be SAT Max. This will help us with sorting later on, if we want. Also, note that sometimes the score is nore reported! So, there will be no numbers or anything. That's why we need to set a ValueError case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "50686851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAT lower range: 1110\n",
      "SAT upper range: 1370\n"
     ]
    }
   ],
   "source": [
    "# Split the text before and after the hyphen\n",
    "sat_range = sat_range.split('–')\n",
    "# Store the numbers in separate variables\n",
    "try:\n",
    "    sat_min = int(sat_range[0])\n",
    "except ValueError:\n",
    "    sat_min = \"N/A\"\n",
    "try:\n",
    "    sat_max = int(sat_range[1])\n",
    "except (ValueError, IndexError):\n",
    "    sat_max = \"N/A\"\n",
    "# Print the sat ranges\n",
    "print(f'SAT lower range: {sat_min}')\n",
    "print(f'SAT upper range: {sat_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ee5f0",
   "metadata": {},
   "source": [
    "Much better. We could always display it like before as a range with a hyphen, but this will allow us to find descriptive statistics easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a39acb",
   "metadata": {},
   "source": [
    "**Classify as Liberal Arts College (LAC)**\n",
    "\n",
    "Next, we want to classify whether the institution is a liberal arts college or not. This is shown in the text field here:\n",
    "\n",
    "<img src = \"./images/lac.jpg\">\n",
    "\n",
    "We highlight that text field, find the css_selector and get the relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f63af7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allegheny College is a small, 4-year, private liberal arts college. This coed college is located in a large town in a suburban setting and is primarily a residential campus. It offers bachelor's degrees.\n"
     ]
    }
   ],
   "source": [
    "# UNI TYPE\n",
    "uni_type_text = driver.find_element_by_css_selector('.csp-shared-section-descriptions[data-testid=\"csp-more-about-description\"]').text\n",
    "print(uni_type_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08822bb1",
   "metadata": {},
   "source": [
    "Okay, we got all the text in that field, but we're only interested in whether this is a liberal arts college (LAC), so we can do a quick check to make that classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2327dc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni type: LAC\n"
     ]
    }
   ],
   "source": [
    "# Create a variable based on conditions\n",
    "uni_type = \"\"\n",
    "# Classify:\n",
    "if \"private university\" in uni_type_text.lower():\n",
    "    uni_type = \"Private Uni\"\n",
    "elif \"liberal arts college\" in uni_type_text.lower():\n",
    "    uni_type = \"LAC\" \n",
    "print(f'Uni type: {uni_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf8436",
   "metadata": {},
   "source": [
    "### Switch to Admissions Tab\n",
    "We're done with the About page. We got all the infor we want from there for now. Let's switch to Admissions tab next. But first, we have to find a way to click on Admissions to switch tab. This can be done easily by first locating the Admissions button and then using `.click()` to click on it!\n",
    "\n",
    "As this is a link that has a specific text on it, we can actually find this using `find_element_by_link_text()` as follows. If you run thw following script, you'll notice that your selenium chrome window has switched to the Admissions tab by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e7cfc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH TO ADMISSIONS TAB\n",
    "admission_link = driver.find_element_by_link_text(\"Admissions\")\n",
    "# Click on the element\n",
    "admission_link.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd30d9",
   "metadata": {},
   "source": [
    "**Acceptance Rate, Selectivity, Dates**\n",
    "\n",
    "For this section, I won't go into details, as the method is largely  the same, and we're using more or less the same codes as above. Except for Acceptance Rate and Selectivity. Sometimes this is not reported, so we'll either get a value error or an index error, in those cases, we will store the value of \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e77f46da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptance Rate: 62%\n",
      "Selectivity: Less Selective\n",
      " Regular Action Deadline: Feb 15\n",
      " Early Action Deadline: Dec 1\n",
      " Early Decision Deadline: Dec 1\n"
     ]
    }
   ],
   "source": [
    "# ACCEPTANCE RATE AND SELECTIVITY\n",
    "acceptance_text = driver.find_element_by_css_selector('.cs-label-value-pair-value[data-testid=\"acceptance-rate-value\"]').text\n",
    "# Splitting the text based on the opening parenthesis\n",
    "parts = acceptance_text.split(\" (\")\n",
    "# Storing the values in separate variables\n",
    "try:\n",
    "    acceptance_rate = parts[0]\n",
    "except IndexError:\n",
    "    acceptance_rate = \"N/A\"\n",
    "try:\n",
    "    selectivity = parts[1][:-1]  # Removing the closing parenthesis\n",
    "except (IndexError, ValueError):\n",
    "    selectivity = \"N/A\"\n",
    "print(\"Acceptance Rate:\", acceptance_rate)\n",
    "print(\"Selectivity:\", selectivity)\n",
    "\n",
    "# DATES\n",
    "date_ra = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"regular-decision-value\"]').text\n",
    "date_ea = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-action-value\"]').text\n",
    "date_ed = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-decision-value\"]').text\n",
    "print(f'Regular Action Deadline: {date_ra}')\n",
    "print(f'Early Action Deadline: {date_ea}')\n",
    "print(f'Early Decision Deadline: {date_ea}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4ff4e",
   "metadata": {},
   "source": [
    "### Switch to Costs tab\n",
    "\n",
    "Just like before, we can inspect the element of `Costs` and find out more information about it and use the similar code as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "85758c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SWITCH TO COSTS TAB\n",
    "costs_link = driver.find_element_by_link_text(\"Costs\")\n",
    "# Click on the element\n",
    "costs_link.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07fb20",
   "metadata": {},
   "source": [
    "**Net Cost, Tuition (Out of State), Housing, Books, Personal Expenses**\n",
    "\n",
    "From this tab, we're interested in the average net price, which is the cost of tuition and fees minus scholarships and grants. We're also interested in costs such as for housing, books and supplies, and personal expenses. And, finally, we're interested in tuition, with a catch. For this particular example, we're only intersted in Out-of-state tuition. You'll notice that some of the url's will only list Tuition with id `csp-list-item-private-tuition-value`, but not in-state and out-of-state separate. While other url's will separate those; id = `csp-list-item-out-of-state-tuition-value`. We have to consider that! \n",
    "\n",
    "As with the previous section, won't go into details on some of these. Notice that we use filters to only keep the number values! But sometimes, the uni's don't list any numbers at all, it may be \"Not Available\", so we need create a function to deal with such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c46e73",
   "metadata": {},
   "source": [
    "The `filter()` function is used to iterate over each character in the books string and apply the `str.isdigit()` function to filter out only the characters that are digits (0-9). It returns an iterable containing only the digit characters from the books string.\n",
    "\n",
    "The join() method concatenates the filtered digit characters from the iterable into a single string. The '' empty string is used as the separator, meaning the digit characters will be joined without any separators between them.\n",
    "\n",
    "We then check whether digit_string exists; if it does, we store that value. Otherwise, we store empty value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "15dabb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the costs section:\n",
    "def get_numeric_val(var):\n",
    "    filtered_digits = filter(str.isdigit, var)\n",
    "    digit_string = ''.join(filtered_digits)\n",
    "    if digit_string:\n",
    "        var = int(digit_string)\n",
    "    else:\n",
    "        var = ''  \n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "79d8223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Cost: 24227\n",
      "Housing Cost: 14868\n",
      "Books and Supplies Cost: 400\n",
      "Personal Expenses: 1100\n",
      "Avg Need met: 90%\n"
     ]
    }
   ],
   "source": [
    "# COSTS\n",
    "net_cost = driver.find_element_by_class_name(\"csp-tuition-label-value-pair-value\").text\n",
    "net_cost = get_numeric_val(net_cost)\n",
    "\n",
    "housing = driver.find_element_by_id(\"csp-list-item-average-housing-value\").text\n",
    "housing = get_numeric_val(housing)\n",
    "\n",
    "books = driver.find_element_by_id(\"csp-list-item-books-and-supplies-value\").text\n",
    "books = get_numeric_val(books)\n",
    "\n",
    "personal_exp = driver.find_element_by_id(\"csp-list-item-estimatedPersonalExpenses-value\").text\n",
    "personal_exp = get_numeric_val(personal_exp)\n",
    "\n",
    "print(f'Net Cost: {net_cost}')\n",
    "print(f'Housing Cost: {housing}')\n",
    "print(f'Books and Supplies Cost: {books}')\n",
    "print(f'Personal Expenses: {personal_exp}')\n",
    "\n",
    "# AVG FINANCIAL NEED MET\n",
    "need_met = driver.find_element_by_id(\"csp-list-item-need-met-value\").text\n",
    "print(f'Avg Need met: {need_met}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c75ee0",
   "metadata": {},
   "source": [
    "**Tuition**\n",
    "As mentioned above, tuition is a bit trickier, private unis will have only tuition listed, whereas public unis may have out-of-state listed separately. So we have to consider both cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ebc82304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuition: 54300\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tuition = driver.find_element_by_id(\"csp-list-item-private-tuition-value\").text\n",
    "    tuition = get_numeric_val(tuition)\n",
    "except NoSuchElementException:\n",
    "    try:\n",
    "        tuition = driver.find_element_by_id(\"csp-list-item-out-of-state-tuition-value\").text\n",
    "        tuition = get_numeric_val(tuition)\n",
    "    except NoSuchElementException:\n",
    "        tuition = \"N/A\"\n",
    "print(f'Tuition: {tuition}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfcf58d",
   "metadata": {},
   "source": [
    "And we're done with the BigFuture site! That's all the information we want to scrape for now. So let's see if we can run all that in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb6a48",
   "metadata": {
    "tags": [
     "bf-single"
    ]
   },
   "source": [
    "## SCRAPE SINGLE URL - BIGFUTURE<a id='bf-single'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "402000c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "50bcf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the costs section:\n",
    "def get_numeric_val(var):\n",
    "    filtered_digits = filter(str.isdigit, var)\n",
    "    digit_string = ''.join(filtered_digits)\n",
    "    if digit_string:\n",
    "        var = int(digit_string)\n",
    "    else:\n",
    "        var = ''  \n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "15d48c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University Name: Harvard College\n",
      "City: Cambridge\n",
      "State: MA\n",
      "No of years required: 4\n",
      "Uni Control: Private\n",
      "Uni Size: Medium\n",
      "Uni Setting: Urban\n",
      "SAT Lower Range: N/A\n",
      "SAT Upper Range: N/A\n",
      "Uni Type: Private Uni\n",
      "Switching to Admissions tab.\n",
      "Acceptance Rate: 4%\n",
      "Selectivity: Extremely Selective\n",
      "Regular Action Deadline: Jan 1\n",
      "Early Action Deadline: Nov 1\n",
      "Early Decision Deadline: Not available\n",
      "Switching to Costs tab\n",
      "Net Cost: \n",
      "Tuition: 54269\n",
      "Books: 1000\n",
      "Personal Expenses: 2500\n",
      "Average Financial Need Met: 100%\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE FOR SINGLE SITE. UNCOMMENT BEFORE RUNNING!\n",
    "\n",
    "DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.get(\"https://bigfuture.collegeboard.org/colleges/harvard-college\")\n",
    "# Use line 7 to test whether it works fine for out-of-state tuition\n",
    "# driver.get(\"https://bigfuture.collegeboard.org/colleges/minnesota-state-university-moorhead/\")\n",
    "\n",
    "time.sleep(2) # Add a small delay before clicking on button\n",
    "try:\n",
    "    cookies_button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "    driver.execute_script(\"arguments[0].click();\", cookies_button)\n",
    "except NoSuchElementException:\n",
    "    pass  # Skip clicking if the element is not found\n",
    "\n",
    "time.sleep(1) # Add a small delay after clicking on button\n",
    "\n",
    "#UNI NAME\n",
    "uni_name = driver.find_element_by_id(\"csp-banner-section-school-name-label\").text\n",
    "print(f'University Name: {uni_name}')\n",
    "\n",
    "# STATE AND CITY\n",
    "location = driver.find_element_by_id(\"csp-banner-section-school-location-label\").text\n",
    "# Splitting the text based on the comma\n",
    "parts = location.split(',')\n",
    "# Removing any leading or trailing whitespace\n",
    "city_name = parts[0].strip()\n",
    "print(f'City: {city_name}')\n",
    "# Extracting the last two letters\n",
    "state_code = parts[1].strip()\n",
    "print(f'State: {state_code}')\n",
    "\n",
    "# NO. OF YEARS\n",
    "no_of_years = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-years\"]').text\n",
    "no_of_years = re.search(r'\\d+', no_of_years).group()\n",
    "print(f'No of years required: {no_of_years}')\n",
    "\n",
    "# UNI CONTROL\n",
    "uni_ctrl = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-designation\"]').text\n",
    "print(f'Uni Control: {uni_ctrl}')\n",
    "\n",
    "# UNI SIZE AND SETTING\n",
    "uni_size = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-size\"]').text\n",
    "print(f'Uni Size: {uni_size}')\n",
    "uni_setting = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-setting\"]').text\n",
    "print(f'Uni Setting: {uni_setting}')\n",
    "\n",
    "# SAT SCORES\n",
    "sat_range = driver.find_element_by_css_selector('.csp-overview-info-text[data-testid=\"csp-overview-sat-range\"]').text\n",
    "#Get the text of the element\n",
    "sat_range = sat_range.split('–')\n",
    "# Storing the numbers in separate variables\n",
    "try:\n",
    "    sat_min = int(sat_range[0])\n",
    "except ValueError:\n",
    "    sat_min = \"N/A\"\n",
    "try:\n",
    "    sat_max = int(sat_range[1])\n",
    "except (ValueError, IndexError):\n",
    "    sat_max = \"N/A\"\n",
    "# Print the sat ranges\n",
    "print(f'SAT Lower Range: {sat_min}')\n",
    "print(f'SAT Upper Range: {sat_max}')\n",
    "\n",
    "# UNI TYPE\n",
    "uni_type_text = driver.find_element_by_css_selector('.csp-shared-section-descriptions[data-testid=\"csp-more-about-description\"]').text\n",
    "# Create a variable based on conditions\n",
    "uni_type = \"\"\n",
    "if \"private university\" in uni_type_text:\n",
    "    uni_type = \"Private Uni\"\n",
    "elif \"liberal arts college\" in uni_type_text:\n",
    "    uni_type = \"LAC\" \n",
    "print(f'Uni Type: {uni_type}')\n",
    "\n",
    "\n",
    "## SWITCH TO ADMISSIONS TAB\n",
    "print('Switching to Admissions tab.')\n",
    "admission_link = driver.find_element_by_link_text(\"Admissions\")\n",
    "# Click on the element\n",
    "admission_link.click()\n",
    "\n",
    "# ACCEPTANCE RATE AND SELECTIVITY\n",
    "acceptance_text = driver.find_element_by_css_selector('.cs-label-value-pair-value[data-testid=\"acceptance-rate-value\"]').text\n",
    "# Splitting the text based on the opening parenthesis\n",
    "parts = acceptance_text.split(\" (\")\n",
    "# Storing the values in separate variables\n",
    "try:\n",
    "    acceptance_rate = parts[0]\n",
    "except IndexError:\n",
    "    acceptance_rate = \"N/A\"\n",
    "try:\n",
    "    selectivity = parts[1][:-1]  # Removing the closing parenthesis\n",
    "except (IndexError, ValueError):\n",
    "    selectivity = \"N/A\"\n",
    "print(\"Acceptance Rate:\", acceptance_rate)\n",
    "print(\"Selectivity:\", selectivity)\n",
    "\n",
    "# DATES\n",
    "date_ra = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"regular-decision-value\"]').text\n",
    "date_ea = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-action-value\"]').text\n",
    "date_ed = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-decision-value\"]').text\n",
    "print(f'Regular Action Deadline: {date_ra}')\n",
    "print(f'Early Action Deadline: {date_ea}')\n",
    "print(f'Early Decision Deadline: {date_ed}')\n",
    "\n",
    "## SWITCH TO COSTS TAB\n",
    "print('Switching to Costs tab')\n",
    "costs_link = driver.find_element_by_link_text(\"Costs\")\n",
    "# Click on the element\n",
    "costs_link.click()\n",
    "\n",
    "# COSTS\n",
    "net_cost = driver.find_element_by_class_name(\"csp-tuition-label-value-pair-value\").text\n",
    "net_cost = get_numeric_val(net_cost)\n",
    "housing = driver.find_element_by_id(\"csp-list-item-average-housing-value\").text\n",
    "housing = get_numeric_val(housing)\n",
    "books = driver.find_element_by_id(\"csp-list-item-books-and-supplies-value\").text\n",
    "books = get_numeric_val(books)\n",
    "personal_exp = driver.find_element_by_id(\"csp-list-item-estimatedPersonalExpenses-value\").text\n",
    "personal_exp = get_numeric_val(personal_exp)\n",
    "try:\n",
    "    tuition = driver.find_element_by_id(\"csp-list-item-private-tuition-value\").text\n",
    "    tuition = get_numeric_val(tuition)\n",
    "except NoSuchElementException:\n",
    "    try:\n",
    "        tuition = driver.find_element_by_id(\"csp-list-item-out-of-state-tuition-value\").text\n",
    "        tuition = get_numeric_val(tuition)\n",
    "    except NoSuchElementException:\n",
    "        tuition = None  # or any default value you prefer\n",
    "print(f'Net Cost: {net_cost}')\n",
    "print(f'Tuition: {tuition}')\n",
    "print(f'Books: {books}')\n",
    "print(f'Personal Expenses: {personal_exp}')\n",
    "\n",
    "# AVG FINANCIAL NEED MET\n",
    "need_met = driver.find_element_by_id(\"csp-list-item-need-met-value\").text\n",
    "print(f'Average Financial Need Met: {need_met}')\n",
    "\n",
    "# End session\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bdc7c",
   "metadata": {},
   "source": [
    "The above code works just fine for a single url, and we were able to get the information we need! Now it's time to test it on multiple webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433b29a",
   "metadata": {
    "tags": [
     "bf-multiple"
    ]
   },
   "source": [
    "## SCRAPE MULTIPLE URLS - BIGFUTURE<a id='bf-multi'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc21cd",
   "metadata": {},
   "source": [
    "First, we need to create a list of the urls. I have a .txt file with all the urls which I'll parse through and create a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "24443862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ead929c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list\n",
    "site_lists = []\n",
    "\n",
    "# Open the text file and read the links\n",
    "with open('uni_list_2.txt', 'r') as file:\n",
    "    # Read each line and append the link to the list\n",
    "    for line in file:\n",
    "        link = line.strip()  # Remove leading/trailing whitespaces and newline characters\n",
    "        site_lists.append(link)\n",
    "\n",
    "# Print the list of links\n",
    "# print(site_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93db1c",
   "metadata": {},
   "source": [
    "We define `get_numeric_val` function here again, in case we don't want to re-run previous cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78aa4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the costs section:\n",
    "def get_numeric_val(var):\n",
    "    filtered_digits = filter(str.isdigit, var)\n",
    "    digit_string = ''.join(filtered_digits)\n",
    "    if digit_string:\n",
    "        var = int(digit_string)\n",
    "    else:\n",
    "        var = ''  \n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf0630",
   "metadata": {},
   "source": [
    "We wrap our code from the single url case inside a for loop where we iterate through each of the url's in the list. Another notable thing is that for each snippet of code, we wrapped it in a **try** and **except** block; this is so that in case we don't find that particular locator, we'll note \"N/A\" for that value and move on. Also, for each snippet of code, we **append** to our ever growing lists, which we will use later to create a pandas dataframe and export as a spreadsheet!\n",
    "\n",
    "There's a lot of commented out print commands, which are crucial for debugging in case we run into errors; they were used heavily during my initial testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "024049ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder cell for testing a small sample of urls or problem urls.\n",
    "#site_lists = ['https://bigfuture.collegeboard.org/colleges/columbia-university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2d141d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current URL: https://bigfuture.collegeboard.org/colleges/columbia-university\n",
      "Trying to click on cookies button\n",
      "Getting Data\n",
      "Data Collected for Columbia University\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store values in\n",
    "uni_name_list = []\n",
    "city_name_list = []\n",
    "state_code_list = []\n",
    "no_of_years_list = []\n",
    "uni_ctrl_list = []\n",
    "uni_size_list = []\n",
    "uni_setting_list = []\n",
    "sat_min_list = []\n",
    "sat_max_list = []\n",
    "uni_type_list = []\n",
    "acceptance_list = []\n",
    "selectivity_list = []\n",
    "date_ra_list = []\n",
    "date_ea_list = []\n",
    "date_ed_list = []\n",
    "net_cost_list = []\n",
    "tuition_list = []\n",
    "housing_list = []\n",
    "books_list = []\n",
    "personal_exp_list = []\n",
    "need_met_list = []\n",
    "\n",
    "# Create a loop to go through each url in the list and scrape the sites\n",
    "for link in site_lists:\n",
    "\n",
    "    print(f'Accessing URL: {link}')\n",
    "    DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options = options, executable_path=DRIVER_PATH)\n",
    "    driver.get(link)\n",
    "    \n",
    "    time.sleep(2)  # Add a delay of 2 seconds (adjust the value as needed)\n",
    "    \n",
    "    # CLICK ACCEPT COOKIES\n",
    "    print('Trying to click on cookies button')\n",
    "    try:\n",
    "        cookies_button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "        driver.execute_script(\"arguments[0].click();\", cookies_button)\n",
    "    except NoSuchElementException:\n",
    "        pass  # Skip clicking if the element is not found\n",
    "    \n",
    "    time.sleep(1) # Add a delay of 1 second after clicking on button\n",
    "    \n",
    "    print(\"Getting Data.\")\n",
    "    \n",
    "    # UNI NAME\n",
    "    try:\n",
    "        uni_name = driver.find_element_by_id(\"csp-banner-section-school-name-label\").text\n",
    "        #print(f'Uni Name: {uni_name}')\n",
    "    except NoSuchElementException:\n",
    "        uni_name = \"N/A\"\n",
    "    uni_name_list.append(uni_name)\n",
    "    \n",
    "    # STATE AND CITY\n",
    "    try:\n",
    "        location = driver.find_element_by_id(\"csp-banner-section-school-location-label\").text\n",
    "        # Splitting the text based on the comma\n",
    "        parts = location.split(',')\n",
    "        # Removing any leading or trailing whitespace\n",
    "        city_name = parts[0].strip()\n",
    "        # Extracting the last two letters\n",
    "        state_code = parts[1].strip()\n",
    "    except NoSuchElementException:\n",
    "        city_name = \"N/A\"\n",
    "        state_code = \"N/A\"\n",
    "    city_name_list.append(city_name)\n",
    "    #print(f'City: {city_name}')\n",
    "    state_code_list.append(state_code)\n",
    "    #print(f'State: {state_code}')    \n",
    "\n",
    "    # NO. OF YEARS\n",
    "    try:\n",
    "        no_of_years = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-years\"]').text\n",
    "        no_of_years = re.search(r'\\d+', no_of_years).group()\n",
    "    except NoSuchElementException:\n",
    "        no_of_years = \"N/A\"\n",
    "    no_of_years_list.append(no_of_years)\n",
    "    # print(f'No of years required: {no_of_years}')\n",
    "\n",
    "    # UNI CONTROL\n",
    "    try:\n",
    "        uni_ctrl = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-type-by-designation\"]').text\n",
    "    except NoSuchElementException:\n",
    "        uni_ctrl = \"N/A\"\n",
    "    uni_ctrl_list.append(uni_ctrl)\n",
    "    # print(f'Uni Control: {uni_ctrl}')\n",
    "\n",
    "    # UNI SIZE \n",
    "    try:\n",
    "        uni_size = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-size\"]').text\n",
    "    except NoSuchElementException:\n",
    "        uni_size = \"N/A\"\n",
    "    uni_size_list.append(uni_size)\n",
    "    # print(f'Uni Size: {uni_size}')\n",
    "    \n",
    "    # UNI SETTING\n",
    "    try:\n",
    "        uni_setting = driver.find_element_by_css_selector('[data-testid=\"csp-overview-school-setting\"]').text\n",
    "    except NoSuchElementException:\n",
    "        uni_setting = \"N/A\"\n",
    "    uni_setting_list.append(uni_setting)\n",
    "    # print(f'Uni Setting: {uni_setting}')\n",
    "\n",
    "    # SAT SCORES    \n",
    "    try:\n",
    "        sat_range = driver.find_element_by_css_selector('.csp-overview-info-text[data-testid=\"csp-overview-sat-range\"]').text\n",
    "        #Get the text of the element\n",
    "        sat_range = sat_range.split('–')\n",
    "        # Storing the numbers in separate variables\n",
    "        try:\n",
    "            sat_min = int(sat_range[0])\n",
    "        except ValueError:\n",
    "            sat_min = \"N/A\"\n",
    "        try:\n",
    "            sat_max = int(sat_range[1])\n",
    "        except (ValueError, IndexError):\n",
    "            sat_max = \"N/A\"\n",
    "    except NoSuchElementException:\n",
    "        sat_min = \"N/A\"\n",
    "        sat_max = \"N/A\"\n",
    "    sat_min_list.append(sat_min)\n",
    "    sat_max_list.append(sat_max)\n",
    "    # Print the sat ranges\n",
    "    # print(f'SAT Lower Range: {sat_min}')\n",
    "    # print(f'SAT Upper Range: {sat_max}')\n",
    "\n",
    "    # UNI TYPE\n",
    "    try:\n",
    "        uni_type_text = driver.find_element_by_css_selector('.csp-shared-section-descriptions[data-testid=\"csp-more-about-description\"]').text\n",
    "        # Create a variable based on conditions\n",
    "        uni_type = \"\"\n",
    "        if \"private university\" in uni_type_text:\n",
    "            uni_type = \"Private Uni\"\n",
    "        elif \"liberal arts college\" in uni_type_text:\n",
    "            uni_type = \"LAC\" \n",
    "    except NoSuchElementException:\n",
    "        uni_type = \"\"\n",
    "    #print(uni_type)\n",
    "    uni_type_list.append(uni_type)\n",
    "\n",
    "\n",
    "    ## SWITCH TO ADMISSIONS TAB\n",
    "    admission_link = driver.find_element_by_link_text(\"Admissions\")\n",
    "    # Click on the element\n",
    "    admission_link.click()\n",
    "    time.sleep(1) # small delay\n",
    "\n",
    "    # ACCEPTANCE RATE AND SELECTIVITY\n",
    "    try:\n",
    "        acceptance_text = driver.find_element_by_css_selector('.cs-label-value-pair-value[data-testid=\"acceptance-rate-value\"]').text\n",
    "        # Splitting the text based on the opening parenthesis\n",
    "        parts = acceptance_text.split(\" (\")\n",
    "        # Storing the values in separate variables\n",
    "        try:\n",
    "            acceptance_rate = parts[0]\n",
    "        except IndexError:\n",
    "            acceptance_rate = \"N/A\"\n",
    "        try:\n",
    "            selectivity = parts[1][:-1]  # Removing the closing parenthesis\n",
    "        except (IndexError, ValueError):\n",
    "            selectivity = \"N/A\"\n",
    "    except NoSuchElementException:\n",
    "        acceptance_rate = \"N/A\"\n",
    "        selectivity = \"N/A\"\n",
    "    acceptance_list.append(acceptance_rate)\n",
    "    selectivity_list.append(selectivity)\n",
    "    # print(\"Acceptance Rate:\", acceptance_rate)\n",
    "    # print(\"Selectivity:\", selectivity)\n",
    "\n",
    "    # DATES\n",
    "    try:\n",
    "        date_ra = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"regular-decision-value\"]').text\n",
    "    except NoSuchElementException:\n",
    "        date_ra = \"Not available\"\n",
    "    date_ra_list.append(date_ra)\n",
    "    # print(f'Regular Action Deadline: {date_ra}')\n",
    "    \n",
    "    try:\n",
    "        date_ea = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-action-value\"]').text\n",
    "    except NoSuchElementException:\n",
    "        date_ea = \"Not available\"\n",
    "    date_ea_list.append(date_ea)\n",
    "    # print(f'Early Action Deadline: {date_ea}')\n",
    "    \n",
    "    try:\n",
    "        date_ed = driver.find_element_by_css_selector('.csp-application-deadline-item-value[data-testid=\"early-decision-value\"]').text\n",
    "    except NoSuchElementException:\n",
    "        date_ed = \"Not available\"\n",
    "    date_ed_list.append(date_ed)\n",
    "    # print(f'Early Decision Deadline: {date_ed}')\n",
    "\n",
    "\n",
    "    ## SWITCH TO COSTS TAB\n",
    "    costs_link = driver.find_element_by_link_text(\"Costs\")\n",
    "    # Click on the element\n",
    "    costs_link.click()\n",
    "    time.sleep(1) # small delay\n",
    "\n",
    "    # COSTS\n",
    "    try:\n",
    "        net_cost = driver.find_element_by_class_name(\"csp-tuition-label-value-pair-value\").text\n",
    "        net_cost = get_numeric_val(net_cost)\n",
    "    except NoSuchElementException:\n",
    "        net_cost = \"N/A\"\n",
    "    net_cost_list.append(net_cost)\n",
    "    #print(f'Net Cost: {net_cost}')   \n",
    "    \n",
    "    # TUITION\n",
    "    try:\n",
    "        tuition = driver.find_element_by_id(\"csp-list-item-private-tuition-value\").text\n",
    "        tuition = get_numeric_val(tuition)\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            tuition = driver.find_element_by_id(\"csp-list-item-out-of-state-tuition-value\").text\n",
    "            tuition = get_numeric_val(tuition)\n",
    "        except NoSuchElementException:\n",
    "            tuition = \"N/A\" \n",
    "    tuition_list.append(tuition)\n",
    "    #print(f'Tuition: {tuition}')\n",
    "    \n",
    "    try:\n",
    "        housing = driver.find_element_by_id(\"csp-list-item-average-housing-value\").text\n",
    "        housing = get_numeric_val(housing)\n",
    "    except NoSuchElementException:\n",
    "        housing = \"N/A\"\n",
    "    housing_list.append(housing)\n",
    "    #print(f'Housing: {housing}')\n",
    "    \n",
    "    try:\n",
    "        books = driver.find_element_by_id(\"csp-list-item-books-and-supplies-value\").text\n",
    "        books = get_numeric_val(books)\n",
    "    except NoSuchElementException:\n",
    "        books = \"N/A\"\n",
    "    books_list.append(books)\n",
    "    #print(f'Books: {books}')\n",
    "    \n",
    "    try:\n",
    "        personal_exp = driver.find_element_by_id(\"csp-list-item-estimatedPersonalExpenses-value\").text\n",
    "        personal_exp = get_numeric_val(personal_exp)\n",
    "    except NoSuchElementException:\n",
    "        personal_exp = \"N/A\"\n",
    "    personal_exp_list.append(personal_exp)\n",
    "    #print(f'Personal Expenses: {personal_exp}')\n",
    "\n",
    "    # AVG FINANCIAL NEED MET\n",
    "    try:\n",
    "        need_met = driver.find_element_by_id(\"csp-list-item-need-met-value\").text\n",
    "        need_met_list.append(need_met)\n",
    "    except NoSuchElementException:\n",
    "        need_met = \"N/A\"\n",
    "    #print(f'Average Financial Need Met: {need_met}')\n",
    "    \n",
    "    # END SESSION\n",
    "    driver.quit()\n",
    "    \n",
    "    print(f\"Data Collected for {uni_name}\")\n",
    "    \n",
    "    # Small delay before getting new data\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062caaa6",
   "metadata": {},
   "source": [
    "**OH NO! The script crashed, what do I do?**\n",
    "\n",
    "In the version it is in right now, the script should run fine, but in case you still do run into an error because some information was missing from a site or any other reason, and your loop crashes somewhere in the middle, fear not! You don't have to rerun everything from scratch. (In fact, this has happened to me a few times already)\n",
    "\n",
    "What you would be tempted to do is go to the next section and create a dataframe with the data that you have already collected so far. And that is fine, but do make sure that the lists are all the same length. You can check that by running `len(list_name)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c08df6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 35)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(personal_exp_list), len(uni_name_list) # and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730bb47",
   "metadata": {},
   "source": [
    "What will happen most likely is some of the first few lists will most likely have a an extra value (usually a N/A), so their length will be 1 more than the remainder of the lists. This means that these first few lists have the values from the url where it crashed. We simply have to delete an element from those lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "358db023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', 'n/a']\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "# Deleting last element from a list\n",
    "a = ['1', '2', '3', 'n/a']\n",
    "print(a)\n",
    "del a[-1]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5170bb",
   "metadata": {},
   "source": [
    "Once you've ensured that all the lists are of the same length, you can proceed to the next section and create a datarame and export to csv. When we rerun the above loop to scrape data, you'll need to modify the url list. You don't want to rerun the url's you already have data for, so make sure those are deleted (or index properly when you run your loop).\n",
    "\n",
    "This way, you'll probably end up with multiple csv's (assuming 1 or more times it crashed). All you have to do is concat them together at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de386c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample Code to concat dataframes\n",
    "# import pandas as pd\n",
    "# # assume your dataframes are called df1 and df2\n",
    "# df_master = pd.concat([df1, df2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b5f66",
   "metadata": {
    "tags": [
     "bf-pandas"
    ]
   },
   "source": [
    "## CREATE DATAFRAME FOR BIGFUTURE<a id='bf-pandas'></a>\n",
    "\n",
    "\n",
    "Here we use `pandas` to create a dataframe using the information stored in all our lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "95420bbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Years</th>\n",
       "      <th>Control</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Setting</th>\n",
       "      <th>SAT Min</th>\n",
       "      <th>SAT Max</th>\n",
       "      <th>...</th>\n",
       "      <th>RA</th>\n",
       "      <th>EA</th>\n",
       "      <th>ED</th>\n",
       "      <th>Net Cost</th>\n",
       "      <th>Tuition</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal Expenses</th>\n",
       "      <th>Avg Need Met</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Columbia University</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>4</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private Uni</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1500</td>\n",
       "      <td>1560</td>\n",
       "      <td>...</td>\n",
       "      <td>Jan 1</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Nov 1</td>\n",
       "      <td>21828</td>\n",
       "      <td>65340</td>\n",
       "      <td>16800</td>\n",
       "      <td>1392</td>\n",
       "      <td>2350</td>\n",
       "      <td>99%</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       University Name      City State Years  Control         Type    Size  \\\n",
       "0  Columbia University  New York    NY     4  Private  Private Uni  Medium   \n",
       "\n",
       "  Setting  SAT Min  SAT Max  ...     RA             EA     ED Net Cost  \\\n",
       "0   Urban     1500     1560  ...  Jan 1  Not available  Nov 1    21828   \n",
       "\n",
       "  Tuition  Housing  Books  Personal Expenses  Avg Need Met  \\\n",
       "0   65340    16800   1392               2350           99%   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://bigfuture.collegeboard.org/colleges/co...  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"University Name\": uni_name_list,\n",
    "    \"City\": city_name_list,\n",
    "    \"State\": state_code_list,\n",
    "    \"Years\": no_of_years_list,\n",
    "    \"Control\": uni_ctrl_list,\n",
    "    \"Type\": uni_type_list,\n",
    "    \"Size\": uni_size_list,\n",
    "    \"Setting\": uni_setting_list,\n",
    "    \"SAT Min\": sat_min_list,\n",
    "    \"SAT Max\": sat_max_list,\n",
    "    \"Acceptance Rate\": acceptance_list,\n",
    "    \"Difficulty Level\": selectivity_list,\n",
    "    \"RA\": date_ra_list,\n",
    "    \"EA\": date_ea_list,\n",
    "    \"ED\": date_ed_list,\n",
    "    \"Net Cost\": net_cost_list,\n",
    "    \"Tuition\": tuition_list,\n",
    "    \"Housing\": housing_list,\n",
    "    \"Books\": books_list,\n",
    "    \"Personal Expenses\": personal_exp_list,\n",
    "    \"Avg Need Met\": need_met_list,\n",
    "    \"URL\" : site_lists\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "bigfuture_info = pd.DataFrame(data)\n",
    "\n",
    "bigfuture_info.sample(5) # display 5 random rows from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4d216",
   "metadata": {},
   "source": [
    "**SAVE BIGFUTURE DATAFRAME TO CSV**\n",
    "\n",
    "If we are interested only in the BigFuture data, we can now save a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e50ed5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "bigfuture_info.to_csv('bigfuture_uni_info_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ee3bb",
   "metadata": {},
   "source": [
    "And we're done!! But, are we? We still have to scrape the CollegeData website, so that's what we do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06127e",
   "metadata": {
    "tags": [
     "cd"
    ]
   },
   "source": [
    "## EXPLORE COLLEGEDATA.COM<a id='cd'></a>\n",
    "\n",
    "\n",
    "Just like with the BigFuture website, it's important to explore CollegeData.com and see where the information is laid out, how it's presented, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c15e180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "#The chromedriver_win32 folder is in the same directory where this notebook is located\n",
    "DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "\n",
    "# Initiate the webdriver\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "# This will open a new chrome window\n",
    "driver.get(\"https://waf.collegedata.com/college-search/allegheny-college/\")\n",
    "\n",
    "# Uncomment following to end session and close window\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8dac2a",
   "metadata": {},
   "source": [
    "We notice that there are 6 tabs. The default link opens the Overview tab. There are also Admissions, Financials, Academic, Campus Life, and Students tabs. \n",
    "\n",
    "<img src = \"./images/cd_allegheny.jpg\">\n",
    "\n",
    "After some playing around, we decide to collect the following information.\n",
    "\n",
    "**Overview Tab**:  \n",
    "* University Name\n",
    "* Women %\n",
    "\n",
    "**Admissions Tab**:  \n",
    "* Regular Action Deadline\n",
    "* Early Action Deadline\n",
    "* Early Decision Deadline\n",
    "* Waiting List (accepted or not)\n",
    "* Letters of Recommendation required\n",
    "\n",
    "**Campus Life Tab**:  \n",
    "* Campus Size (in acres)\n",
    "\n",
    "**Students Tab**:\n",
    "* International Students % and how many countries are represented.\n",
    "\n",
    "One thing you'll notice is the annoying popup to subscribe to their newsletter every time you switch to a new tab! We'll need to deal with this first.\n",
    "\n",
    "Another thing to note is that since we're not specifying the window size, Selenium loads the chrome page in default size. This actually makes it so that some of the sections are not automatically expanded. We'll need to script clicking on the section headers to expandion the sections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d151fb",
   "metadata": {},
   "source": [
    "### **FUNCTION TO CLOSE NEWSLETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69141b83",
   "metadata": {},
   "source": [
    "This section writes a function to deal with the pesky newsletter subscription popup that shows up every time we switch tabs. First, we inspect the close button (X on the top right) and find the class name `NewsletterModal_close__2xauP`. This is the same regardless of which tab the popup shows up on, so we can write a single function to close this. Also, note that the popup takes a few seconds to load, so we add a small time delay before we attempt to find and close this popup window. We end the function with a small time delay as well, so that we have a little leeway before we move to next tasks.\n",
    "\n",
    "<img src = \"./images/news_button.jpg\">\n",
    "\n",
    "We will call on the `close_newsletter()` function every time we switch tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6bdbe906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Close newsletter\n",
    "import time\n",
    "\n",
    "def close_newsletter():\n",
    "    time.sleep(5) # small delay as the hidden page takes a few seconds to load\n",
    "    print('Closing Newsletter.')\n",
    "    try:\n",
    "        hidden_button = driver.find_element_by_class_name(\"NewsletterModal_close__2xauP\")\n",
    "        driver.execute_script(\"arguments[0].click();\", hidden_button)\n",
    "    except NoSuchElementException:\n",
    "        pass  # Skip clicking if the element is not found\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5b41ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing Newsletter.\n"
     ]
    }
   ],
   "source": [
    "# Run close_newsletter() to close the popup\n",
    "close_newsletter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17038d9",
   "metadata": {},
   "source": [
    "### Let's start scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66421c6",
   "metadata": {},
   "source": [
    "**University Name**  \n",
    "First, let's get the University name from the Overview page. As before, we highlight the text and Inspect element. There are several ways to go about it. We can use the class `ActionHeader_titles__1bP13`, or we can also just use the xpath. Be careful about using the xpath though, as it does not always work in every url (the path to the information may change). However, I've checked several different url's from CollegeData and the xpath seems to work every time for the University Name, so we'll go with this for now.\n",
    "\n",
    "Copying the element gives this:\n",
    "`<div class=\"ActionHeader_titles__1bP13\"><h1 class=\"cd-web-h1 ActionHeader_title__104Gs\">Allegheny College<span class=\"ActionHeader_subtitle__1i5Tt\">Facts &amp; Information</span></h1></div>`\n",
    "\n",
    "<img src = \"./images/allegheny_class.jpg\">\n",
    "\n",
    "Notice in the code below that we use the `find_element_by_xpath()` function. We get the xpath by highlighting the element and then copying the full xpath and pasting in this cell. And as before, we have to make sure to use the `.text` at the end.\n",
    "\n",
    "A small problem here is that the information we get by using this xpath is the University Name in the first line, followed by \"Facts and Information\" in the second line. We don't care about the second line. So we use `.splitlines()` to split the lines and then only take the first line. Also, in case this xpath doesn't exist, we make sure to handle exceptions `NoSuchElementException` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e96410f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University Name: Allegheny College\n"
     ]
    }
   ],
   "source": [
    "# UNI NAME\n",
    "try:\n",
    "    uni_name = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[1]/div/div/div/div/div[1]/h1').text\n",
    "    uni_name = uni_name.splitlines()[0]\n",
    "except NoSuchElementException:\n",
    "    uni_name = \"N/A\"\n",
    "print(f'University Name: {uni_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89e07d",
   "metadata": {},
   "source": [
    "**Women %**  \n",
    "Next, we want the Women % data as shown in the image below. We inspect the element as usual.\n",
    "<img src = \"./images/women_pct.jpg\">\n",
    "\n",
    "Some problems here! Namely, notice that the class name is not unique. You can do a quick check by copying the class id and searching for it by pressing CTRL + F and pasting the idea; you'll see that there are multiple matches. Now if we were absolutely sure that every single url will have the exact same number of classes and the index of Women % would always be the same, we could just use indexing instead. However, as I found out with some trial and error, that is not always the case for this website. So, we go for xpath again, and I did test this for several sites, and it worked.\n",
    "\n",
    "The result we get is in the form, e.g., `Women - 54.9%%`, we don't want the `Women - ` part of the string, so we'll need to use some manipulation to get rid of that. `.split()` should work fine here, assuming this information is presented the same way for every url. And then we take the second element after the split to get the % we need. We could have also done this with regex, of course, but this seemed simpler. This string manipulation could lead to IndexError or AttributeError in case we don't have the data in the right format or it's not listed, so we handle those exceptions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "3160e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women%: 53.9%\n"
     ]
    }
   ],
   "source": [
    "# WOMEN %\n",
    "try:\n",
    "    women_pct = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[3]/div[2]/div/div/div[5]/div[2]\").text\n",
    "    women_pct = women_pct.split(' - ')[1]\n",
    "except NoSuchElementException:\n",
    "    women_pct = \"N/A\"\n",
    "except (IndexError, AttributeError):\n",
    "    women_pct = \"N/A\"\n",
    "print(f'Women%: {women_pct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82b37c",
   "metadata": {},
   "source": [
    "**Expanding Students Section**  \n",
    "\n",
    "Because of the default window size, the Students section at the bottom is not automatically expanded. We'll need to click on it to expand the section.\n",
    "\n",
    "<img src = \"./images/students_button.jpg\">\n",
    "\n",
    "Again, xpath works for several different url, so this should be fine to do. We use the `.click()` function to click on the button and then we run the `close_newsletter()` function to close the newsletter popup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2878be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICK ON STUDENTS\n",
    "students_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[5]/div[2]/div[2]\")\n",
    "students_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c8a73",
   "metadata": {},
   "source": [
    "**International Students and Countries**  \n",
    "The information we want from this section are the % of international students and how many countries are represented in this instituion.\n",
    "\n",
    "<img src = \"./images/intl_students.jpg\">\n",
    "Again, there is a problem here. The classes are not unique, and using xpath doesn't always work either, because some of the url's could have different xpaths. We have to use some tricks here!\n",
    "\n",
    "First, we find all elements under the class `CollegeProfileContent_content__1hJCl`. This has two subclasses: `TitleValue_title__2-afK` and `TitleValue_value__1JT0d`. Now, if ALL the parent class had both these subclasses, we could use indexing tricks to solve the problem. However, as I noticed with some trial and error, it does not. \n",
    "\n",
    "So, what we'll do instead is iterate through all the title and value elements and find where we have the text \"International Students\" in the title; we'll then get the corresponding value.\n",
    "\n",
    "There are a few issues here:  \n",
    "* Sometimes the data is `Not reported`, in which case we have to assign \"N/A\" to our data.\n",
    "* Sometimes only the number of countries is reported, e.g. \"50 countries.\"\n",
    "* Sometimes only the % is reported, e.g. \"4.2%.\"\n",
    "\n",
    "When we do match the title element to \"International Students\", the value we get is usually in the form, e.g., `4.1% from 56 countries`. We want to separate this data so that we capture only the % value and the raw number for the countries. Regex comes in handy here! Side note: you can always test your regex here: https://regex101.com/\n",
    "\n",
    "For more information on how these regex codes work, please check the **regex demos**:  \n",
    "* Getting %: https://regex101.com/r/RIuS6K/1\n",
    "* Getting the number only while ignoring %: https://regex101.com/r/NAmr0p/1\n",
    "\n",
    "As before, in case `NoSuchElementException` occurs, i.e. we can't find information from this class, we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "504b4aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: 4.2%\n",
      "Case 2: N/A\n",
      "Case 3: N/A\n",
      "Case 4: 4.1%\n",
      "Case 1: 50\n",
      "Case 2: N/A\n",
      "Case 3: 50\n",
      "Case 4: N/A\n"
     ]
    }
   ],
   "source": [
    "# Quick example of regex:\n",
    "\n",
    "import re\n",
    "\n",
    "# Test string\n",
    "test_string_1 = \"4.2% from 50 countries\" # Case 1\n",
    "test_string_2 = \"Not reported\" # Case 2\n",
    "test_string_3 = \"50 countries\" # Case 3\n",
    "test_string_4 = \"4.1%\" # Case 4\n",
    "\n",
    "# Find the numbers, capture dots and numbers that follow and end with %.\n",
    "# If we have Index Error, it means we didn't have the % value.\n",
    "# Case 1\n",
    "try:\n",
    "    test_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", test_string_1)[0][0]\n",
    "except IndexError:\n",
    "    test_rate = \"N/A\"\n",
    "print(f'Case 1: {test_rate}')\n",
    "\n",
    "# Case 2\n",
    "try:\n",
    "    test_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", test_string_2)[0][0]\n",
    "except IndexError:\n",
    "    test_rate = \"N/A\"\n",
    "print(f'Case 2: {test_rate}')\n",
    "\n",
    "# Case 3\n",
    "try:\n",
    "    test_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", test_string_3)[0][0]\n",
    "except IndexError:\n",
    "    test_rate = \"N/A\"\n",
    "print(f'Case 3: {test_rate}')\n",
    "\n",
    "# Case 4\n",
    "try:\n",
    "    test_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", test_string_4)[0][0]\n",
    "except IndexError:\n",
    "    test_rate = \"N/A\"\n",
    "print(f'Case 4: {test_rate}')\n",
    "\n",
    "\n",
    "# Getting countries represented\n",
    "# Case 1\n",
    "try:\n",
    "    test_countries = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', test_string_1)\n",
    "    if test_countries:\n",
    "        test_countries = test_countries[0]\n",
    "    else:\n",
    "        test_countries = \"N/A\"\n",
    "except IndexError:\n",
    "    test_countries = \"N/A\"\n",
    "print(f'Case 1: {test_countries}')\n",
    "\n",
    "# Case 2\n",
    "try:\n",
    "    test_countries = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', test_string_2)\n",
    "    if test_countries:\n",
    "        test_countries = test_countries[0]\n",
    "    else:\n",
    "        test_countries = \"N/A\"\n",
    "except IndexError:\n",
    "    test_countries = \"N/A\"\n",
    "print(f'Case 2: {test_countries}')\n",
    "\n",
    "# Case 3\n",
    "try:\n",
    "    test_countries = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', test_string_3)\n",
    "    if test_countries:\n",
    "        test_countries = test_countries[0]\n",
    "    else:\n",
    "        test_countries = \"N/A\"\n",
    "except IndexError:\n",
    "    test_countries = \"N/A\"\n",
    "print(f'Case 3: {test_countries}')\n",
    "\n",
    "# Case 4\n",
    "try:\n",
    "    test_countries = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', test_string_4)\n",
    "    if test_countries:\n",
    "        test_countries = test_countries[0]\n",
    "    else:\n",
    "        test_countries = \"N/A\"\n",
    "except IndexError:\n",
    "    test_countries = \"N/A\"\n",
    "print(f'Case 4: {test_countries}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287f229",
   "metadata": {},
   "source": [
    "Our reg ex works, and we use the method described above fetch information on international students % and number of countries represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "682e8182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting International students % and number of countries represented.\n",
      "International students %: 4.1%\n",
      "Number of countries: 56\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# INTERNATIONAL STUDENTS % AND NUMBER OF COUNTRIES\n",
    "print('Getting International students % and number of countries represented.')\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        if title_element.text == \"International Students\":\n",
    "            intl = value_element.text\n",
    "            \n",
    "            try:\n",
    "                intl_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", intl)[0][0]\n",
    "            except IndexError:\n",
    "                intl_rate = \"N/A\"\n",
    "            print(f'International students %: {intl_rate}')\n",
    "            \n",
    "            try:\n",
    "                intl_ctrs = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', intl)\n",
    "                if intl_ctrs:\n",
    "                    intl_ctrs = intl_ctrs[0]\n",
    "                else:\n",
    "                    intl_ctrs = \"N/A\"\n",
    "            except IndexError:\n",
    "                intl_ctrs = \"N/A\"\n",
    "            print(f'Number of countries: {intl_ctrs}')\n",
    "\n",
    "            break\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ce834",
   "metadata": {},
   "source": [
    "It works - at least for the link we provided. But I've tested it on other urls' where the information on International students was either 'not reported', only countries, or only %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b95da",
   "metadata": {},
   "source": [
    "### **Swith to Admissions Tab**\n",
    "\n",
    "Now we switch to the Admissions tab: First, we right click and inspect.\n",
    "\n",
    "<img src = \"./images/adm_button.jpg\">\n",
    "\n",
    "Again, we can use the xpath for the different tabs, as they seem to be fixed, but we could have easily just used selector as well. Remember to run `close_newsletter()` after to get rid of that pesky popup. As a reminder, Right click on element, and Copy > full Xpath to get the xpath details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "c2615252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing Newsletter.\n"
     ]
    }
   ],
   "source": [
    "# CLICK ON ADMISSIONS\n",
    "admissions_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[2]/div/img\")\n",
    "admissions_button.click()\n",
    "time.sleep(1)\n",
    "close_newsletter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bee390",
   "metadata": {},
   "source": [
    "**Click on Applying for Admissions**  \n",
    "The information we need is under the section Applying for Admissions, which we need to click on to expand the section.\n",
    "\n",
    "<img src = \"./images/apply_adm.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "148a101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICK ON APPLYING FOR ADMISSIONS\n",
    "applying_adms_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[2]/div[2]/div[2]/div\")\n",
    "applying_adms_button.click()\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd32b81",
   "metadata": {},
   "source": [
    "**DEADLINES, LETTERS REQUIRED, WAIT LIST**\n",
    "\n",
    "From the applying for admission section, we're going to fetch multiple information. \n",
    "* Letters of Recommendation\n",
    "* Waiting List Used\n",
    "* Regular Action Deadline\n",
    "* Early Action Deadline\n",
    "* Early Decision Deadline\n",
    "\n",
    "We inspect each element and notice that they are all under the class `CollegeProfileContent_content__1hJCl`. The subclass `TitleValue_title__2-afK` has the titles such as 'Letters of Recommendation', while the subclass `TitleValue_value__1JT0d` has the values, such as '2 required for all freshmen'. Unlike in the BigFuture website, we can't just refer to a specific class id, so we're going to have to use the same method we used earlier for getting the % of international students.\n",
    "\n",
    "First, we'll fetch all the elements under the class `CollegeProfileContent_content__1hJCl`, then we'll loop through each and check the ``TitleValue_title__2-afK`` to see if it matches with any of our required information. If it does, then we'll store the value in the corresponding variable.\n",
    "\n",
    "On a sidenote, I did notice that that regular admission deadline sometimes had two dates separated by a comma. e.g. It could be written as 'February 15, December 1'. After checking through them it seemed like the second date was usually the early action date, so I decided to remove the part after the comma and keep only the first value. So instead of 'February 15, December 1', I stored only 'February 15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "41075f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting letters required, wait list, and deadlines.\n",
      "Letters of Recommendation: 2\n",
      "Waiting List Used: Yes\n",
      "Regular Admission Deadline: February 15\n",
      "Early Action Deadline: December 1\n",
      "Early Decision Deadline: November 15\n"
     ]
    }
   ],
   "source": [
    "# DEADLINES, LETTERS REQUIRED, WAIT LIST\n",
    "print('Getting letters required, wait list, and deadlines.')\n",
    "\n",
    "# Initialize variables\n",
    "lor = \"N/A\" # letter of recommendation\n",
    "wait = \"N/A\" # wait list used\n",
    "ra = \"N/A\" # regular action deadline\n",
    "ea = \"N/A\" # early action deadline\n",
    "ed = \"N/A\" # early decision deadline\n",
    "\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        # Extract information based on the title element's text\n",
    "        title_text = title_element.text.lower()\n",
    "\n",
    "        if \"letters of recommendation\" in title_text:\n",
    "            lor = value_element.text.split()[0]\n",
    "        elif \"waiting list used\" in title_text:\n",
    "            wait = value_element.text\n",
    "        elif \"regular admission deadline\" in title_text:\n",
    "            ra = value_element.text\n",
    "            # As mentioned earlier, some dates have two components, we only want the component before the comma\n",
    "            ra_parts = ra.split(\",\")  # Split the string at the comma\n",
    "            # Check if the date did indeed have two components or not:\n",
    "            if len(ra_parts) > 1:\n",
    "                ra = ra_parts[0].strip()  # Extract the text before the comma and remove leading/trailing spaces\n",
    "            else:\n",
    "                ra = ra.strip()  # No comma present, use the whole string and remove leading/trailing spaces\n",
    "        elif \"early action deadline\" in title_text:\n",
    "            ea = value_element.text\n",
    "        elif \"early decision deadline\" in title_text:\n",
    "            ed = value_element.text\n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not found\n",
    "\n",
    "# Print the extracted information using f-strings\n",
    "print(f\"Letters of Recommendation: {lor}\")\n",
    "print(f\"Waiting List Used: {wait}\")\n",
    "print(f\"Regular Admission Deadline: {ra}\")\n",
    "print(f\"Early Action Deadline: {ea}\")\n",
    "print(f\"Early Decision Deadline: {ed}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f92147",
   "metadata": {},
   "source": [
    "### **Switch to Campus Tab**\n",
    "\n",
    "Next, we'll switch to Campus tab. We'll use the xpath method to click on this tab, and as always, we need to remember to run the `close_newsletter()` function afterwards.\n",
    "\n",
    "<img src = \"./images/campus_button.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ee60eb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing Newsletter.\n"
     ]
    }
   ],
   "source": [
    "# CLICK ON CAMPUS LIFE\n",
    "campus_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[5]')\n",
    "campus_button.click()\n",
    "time.sleep(1)\n",
    "close_newsletter()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5e37e",
   "metadata": {},
   "source": [
    "**University Area/Size**\n",
    "\n",
    "We want the university size in acres, and this information is stored un the Location and Setting section which needs to be expanded, so we inspect the element, copy the xpath, and click on it as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "456d3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLICK ON LOCATION AND SETTING\n",
    "loc_set_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[1]/div/div[2]/div')\n",
    "loc_set_button.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ecb48cd",
   "metadata": {},
   "source": [
    "Again, we inspect the Campus Size element and write a code very similar to the one we used for getting information on the international students %.\n",
    "\n",
    "<img src = \"./images/campus_size.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "96c5b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Uni Area Size.\n",
      "Uni Area: 566 acres\n"
     ]
    }
   ],
   "source": [
    "# UNI AREA SIZE           \n",
    "print('Getting Uni Area Size.')\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "uni_area = 'N/A'\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        if title_element.text.lower() == \"campus size\":\n",
    "            uni_area = value_element.text\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not foundprint(f'Uni Area Size: {uni_area}')\n",
    "print(f'Uni Area: {uni_area}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0cc0b",
   "metadata": {},
   "source": [
    "And we're  done! That's all the information we want from the CollegeData website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "89ff1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End session\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c84c16",
   "metadata": {
    "tags": [
     "cd-single"
    ]
   },
   "source": [
    "## SCRAPE SINGLE URL - COLLEGEDATA<a id='cd-single'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80b22e",
   "metadata": {},
   "source": [
    "It's time to put everything together and see if we can get all that information in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "398db5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "112821a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing Newsletter.\n",
      "University Name: California Institute of Technology\n",
      "Women %: 44.7%\n",
      "Getting International students % and number of countries represented.\n",
      "International students %: 7.9%\n",
      "Number of countries: N/A\n",
      "Switching to Admissions tab.\n",
      "Closing Newsletter.\n",
      "Getting Letters required, wait list, and deadlines.\n",
      "Letters Required: 2\n",
      "Waiting List Used: Yes\n",
      "Regular Admission Deadline: January 3\n",
      "Early Action Deadline: November 1\n",
      "Early Decision Deadline: N/A\n",
      "Switching to Campus tab.\n",
      "Closing Newsletter.\n",
      "Getting Uni Area Size.\n",
      "Uni Area: 124 acres\n",
      "Completed collecting data for California Institute of Technology.\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE FOR SINGLE SITE\n",
    "\n",
    "DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.get(\"https://www.collegedata.com/college-search/california-institute-of-technology\")\n",
    "close_newsletter()\n",
    "\n",
    "# UNI NAME\n",
    "try:\n",
    "    uni_name = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[1]/div/div/div/div/div[1]/h1').text\n",
    "    uni_name = uni_name.splitlines()[0]\n",
    "except NoSuchElementException:\n",
    "    uni_name = \"N/A\"\n",
    "print(f'University Name: {uni_name}')\n",
    "\n",
    "# WOMEN %\n",
    "try:\n",
    "    women_pct = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[3]/div[2]/div/div/div[5]/div[2]\").text\n",
    "    women_pct = women_pct.split(' - ')[1]\n",
    "except NoSuchElementException:\n",
    "    women_pct = \"N/A\"\n",
    "except (IndexError, AttributeError):\n",
    "    women_pct = \"N/A\"\n",
    "print(f'Women %: {women_pct}')\n",
    "\n",
    "# CLICK ON STUDENTS\n",
    "students_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[5]/div[2]/div[2]\")\n",
    "students_button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# INTERNATIONAL STUDENTS % AND NUMBER OF COUNTRIES\n",
    "print('Getting International students % and number of countries represented.')\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        if title_element.text == \"International Students\":\n",
    "            intl = value_element.text\n",
    "            \n",
    "            try:\n",
    "                intl_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", intl)[0][0]\n",
    "            except IndexError:\n",
    "                intl_rate = \"N/A\"\n",
    "            print(f'International students %: {intl_rate}')\n",
    "            \n",
    "            try:\n",
    "                intl_ctrs = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', intl)\n",
    "                if intl_ctrs:\n",
    "                    intl_ctrs = intl_ctrs[0]\n",
    "                else:\n",
    "                    intl_ctrs = \"N/A\"\n",
    "            except IndexError:\n",
    "                intl_ctrs = \"N/A\"\n",
    "            print(f'Number of countries: {intl_ctrs}')\n",
    "            \n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not found\n",
    "\n",
    "# CLICK ON ADMISSIONS\n",
    "print('Switching to Admissions tab.')\n",
    "admissions_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[2]/div/img\")\n",
    "admissions_button.click()\n",
    "time.sleep(1)\n",
    "close_newsletter()\n",
    "\n",
    "# CLICK ON APPLYING FOR ADMISSIONS\n",
    "applying_adms_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[2]/div[2]/div[2]/div\")\n",
    "applying_adms_button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# GET LETTERS REQUIURED, WAIT LIST, AND DEADLINES\n",
    "print('Getting Letters required, wait list, and deadlines.')\n",
    "# Initialize variables\n",
    "lor = \"N/A\"\n",
    "wait = \"N/A\"\n",
    "ra = \"N/A\"\n",
    "ea = \"N/A\"\n",
    "ed = \"N/A\"\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        # Extract information based on the title element's text\n",
    "        title_text = title_element.text.lower()\n",
    "\n",
    "        if \"letters of recommendation\" in title_text:\n",
    "            lor = value_element.text.split()[0]\n",
    "        elif \"waiting list used\" in title_text:\n",
    "            wait = value_element.text\n",
    "        elif \"regular admission deadline\" in title_text:\n",
    "            ra = value_element.text\n",
    "            ra_parts = ra.split(\",\")  # Split the string at the comma\n",
    "            if len(ra_parts) > 1:\n",
    "                ra = ra_parts[0].strip()  # Extract the text before the comma and remove leading/trailing spaces\n",
    "            else:\n",
    "                ra = ra.strip()  # No comma present, use the whole string and remove leading/trailing spaces\n",
    "        elif \"early action deadline\" in title_text:\n",
    "            ea = value_element.text\n",
    "        elif \"early decision deadline\" in title_text:\n",
    "            ed = value_element.text\n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not found\n",
    "# Print the extracted information using f-strings\n",
    "print(f\"Letters Required: {lor}\")\n",
    "print(f\"Waiting List Used: {wait}\")\n",
    "print(f\"Regular Admission Deadline: {ra}\")\n",
    "print(f\"Early Action Deadline: {ea}\")\n",
    "print(f\"Early Decision Deadline: {ed}\")\n",
    "\n",
    "# CLICK ON CAMPUS LIFE\n",
    "print('Switching to Campus tab.')\n",
    "campus_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[5]')\n",
    "campus_button.click()\n",
    "time.sleep(1)\n",
    "close_newsletter()     \n",
    "           \n",
    "# CLICK ON LOCATION AND SETTING\n",
    "loc_set_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[1]/div/div[2]/div')\n",
    "loc_set_button.click()\n",
    "time.sleep(1)\n",
    "           \n",
    "# UNI AREA SIZE           \n",
    "print('Getting Uni Area Size.')\n",
    "# Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "uni_area = 'N/A'\n",
    "parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "# Iterate over the parent elements and extract the desired information\n",
    "for parent_element in parent_elements:\n",
    "    try:\n",
    "        title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "        value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "        if title_element.text.lower() == \"campus size\":\n",
    "            uni_area = value_element.text\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        continue  # Skip this iteration if one of the elements is not foundprint(f'Uni Area Size: {uni_area}')\n",
    "print(f'Uni Area: {uni_area}')\n",
    "\n",
    "# END SESSION\n",
    "driver.quit()\n",
    "print(f'Completed collecting data for {uni_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b0e9e",
   "metadata": {
    "tags": [
     "cd-multiple"
    ]
   },
   "source": [
    "## SCRAPE MULTIPLE URLS - COLLEGEDATA<a id='cd-multi'></a>\n",
    "\n",
    "\n",
    "We're now ready to wrap our code into a loop where we iterate through a list of url's and scrape each url for the information we need. The information is appended to lists for each of the variable, which we use later for dataframe creation.\n",
    "\n",
    "The text files containing the lists are in the same directory as the notebook, so make sure to write the correct path for your files.\n",
    "\n",
    "Just like with the BigFuture webt scraping script, we wrap our code in a loop, we initialize empty lists which we append to with each iteration, and make sure to handle exceptions. Most of the print commands have been commented out, but they can be used for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8cbcb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8a63d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list\n",
    "site_lists = []\n",
    "\n",
    "# Open the text file and read the links\n",
    "with open('cd_list_2.txt', 'r') as file:\n",
    "    # Read each line and append the link to the list\n",
    "    for line in file:\n",
    "        link = line.strip()  # Remove leading/trailing whitespaces and newline characters\n",
    "        site_lists.append(link)\n",
    "\n",
    "# Print the list of links\n",
    "# print(site_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bb375f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Close newsletter\n",
    "def close_newsletter():\n",
    "    time.sleep(5) # small delay as the hidden page takes a few seconds to load\n",
    "    #print('Closing Newsletter.')\n",
    "    try:\n",
    "        hidden_button = driver.find_element_by_class_name(\"NewsletterModal_close__2xauP\")\n",
    "        driver.execute_script(\"arguments[0].click();\", hidden_button)\n",
    "    except NoSuchElementException:\n",
    "        pass  # Skip clicking if the element is not found\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "8bfc0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of URLS to fetch data from\n",
    "# site_lists = [\"https://waf.collegedata.com/college-search/allegheny-college/\",\n",
    "#              \"https://waf.collegedata.com/college-search/beloit-college\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ea85c02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing URL: https://waf.collegedata.com/college-search/allegheny-college/\n",
      "Getting university name.\n",
      "Getting % of women.\n",
      "Getting International students % and number of countries represented.\n",
      "Clicking on Admissions.\n",
      "Clicking on Applying for Admissions.\n",
      "Getting Letters Required, Wait List, and Deadlines.\n",
      "Clicking on Campus Button.\n",
      "Clicking on Location and Setting button.\n",
      "Getting Uni Area Size.\n",
      "Completed getting info for Allegheny College. Waiting for next link.\n",
      "Accessing URL: https://waf.collegedata.com/college-search/beloit-college\n",
      "Getting university name.\n",
      "Getting % of women.\n",
      "Getting International students % and number of countries represented.\n",
      "Clicking on Admissions.\n",
      "Clicking on Applying for Admissions.\n",
      "Getting Letters Required, Wait List, and Deadlines.\n",
      "Clicking on Campus Button.\n",
      "Clicking on Location and Setting button.\n",
      "Getting Uni Area Size.\n",
      "Completed getting info for all the universities.\n"
     ]
    }
   ],
   "source": [
    "# Create Empty Lists to store values in:\n",
    "uni_name_list = [] # university name list\n",
    "women_pct_list = [] # women % list\n",
    "intl_rate_list = [] # international students rate list\n",
    "intl_ctrs_list = [] # countries represented list\n",
    "ra_list = [] # regular action deadline list\n",
    "ea_list = [] # early action deadline list\n",
    "ed_list = [] # early decision deadline list\n",
    "wait_list = [] # wait list accepted list\n",
    "lor_list = [] # letters required list\n",
    "uni_area_list = [] # university campus size list\n",
    "\n",
    "for link in site_lists:\n",
    "\n",
    "    print(f'Accessing URL: {link}')\n",
    "    DRIVER_PATH = './chromedriver_win32/chromedriver.exe' \n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options = options, executable_path=DRIVER_PATH)\n",
    "    driver.get(link)\n",
    "\n",
    "    close_newsletter()\n",
    "    \n",
    "    # UNI NAME\n",
    "    print('Getting university name.')\n",
    "    try:\n",
    "        uni_name = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[1]/div/div/div/div/div[1]/h1').text\n",
    "        uni_name = uni_name.splitlines()[0]\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        uni_name = \"N/A\"\n",
    "    #print(f'University Name: {uni_name}')\n",
    "    uni_name_list.append(uni_name)\n",
    "    \n",
    "    # WOMEN %\n",
    "    print('Getting % of women.')\n",
    "    try:\n",
    "        women_pct = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[3]/div[2]/div/div/div[5]/div[2]\").text\n",
    "        women_pct = women_pct.split(' - ')[1]\n",
    "    except NoSuchElementException:\n",
    "        women_pct = \"N/A\"\n",
    "    except (IndexError, AttributeError):\n",
    "        women_pct = \"N/A\"\n",
    "    women_pct_list.append(women_pct)\n",
    "#     print(f'Women%: {women_pct}')\n",
    "\n",
    "    # CLICK ON STUDENTS\n",
    "    #print('Clicking on Students button.')\n",
    "    students_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[5]/div[2]/div[2]\")\n",
    "    students_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # INTERNATIONAL STUDENTS % AND NUMBER OF COUNTRIES\n",
    "    print('Getting International students % and number of countries represented.')\n",
    "    # Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "    parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "    # Iterate over the parent elements and extract the desired information\n",
    "    for parent_element in parent_elements:\n",
    "        try:\n",
    "            title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "            value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "\n",
    "            if title_element.text == \"International Students\":\n",
    "                intl = value_element.text\n",
    "                \n",
    "                try:\n",
    "                    intl_rate = re.findall(r\"(\\d+(\\.\\d+)?%)\", intl)[0][0]\n",
    "                except IndexError:\n",
    "                    intl_rate = \"N/A\"\n",
    "                intl_rate_list.append(intl_rate)\n",
    "#                 print(f'International students %: {intl_rate}')\n",
    "\n",
    "                try:\n",
    "                    intl_ctrs = re.findall(r'[-+]?\\b(?!\\d+(?:[,.]\\d+)?%)\\d+(?:[.,]\\d+)?', intl)\n",
    "                    if intl_ctrs:\n",
    "                        intl_ctrs = intl_ctrs[0]\n",
    "                    else:\n",
    "                        intl_ctrs = \"N/A\"\n",
    "                except IndexError:\n",
    "                    intl_ctrs = \"N/A\"\n",
    "                intl_ctrs_list.append(intl_ctrs)\n",
    "#                 print(f'Number of countries: {intl_ctrs}')                \n",
    "                \n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            continue  # Skip this iteration if one of the elements is not found\n",
    "      \n",
    "    # CLICK ON ADMISSIONS\n",
    "    print('Clicking on Admissions.')\n",
    "    admissions_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[2]/div/img\")\n",
    "    admissions_button.click()\n",
    "    time.sleep(1)\n",
    "    close_newsletter()\n",
    "\n",
    "    # CLICK ON APPLYING FOR ADMISSIONS\n",
    "    print('Clicking on Applying for Admissions.')\n",
    "    applying_adms_button = driver.find_element_by_xpath(\"/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[2]/div[2]/div[2]/div\")\n",
    "    applying_adms_button.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # GET LETTERS REQUIURED, WAIT LIST, AND DEADLINES\n",
    "    print('Getting Letters Required, Wait List, and Deadlines.')\n",
    "    # Initialize variables\n",
    "    lor = \"N/A\"\n",
    "    wait = \"N/A\"\n",
    "    ra = \"N/A\"\n",
    "    ea = \"N/A\"\n",
    "    ed = \"N/A\"\n",
    "    # Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "    parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "    # Iterate over the parent elements and extract the desired information\n",
    "    for parent_element in parent_elements:\n",
    "        try:\n",
    "            title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "            value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "            # Extract information based on the title element's text\n",
    "            title_text = title_element.text.lower()\n",
    "            if \"letters of recommendation\" in title_text:\n",
    "                lor = value_element.text.split()[0]\n",
    "            elif \"waiting list used\" in title_text:\n",
    "                wait = value_element.text\n",
    "            elif \"regular admission deadline\" in title_text:\n",
    "                ra = value_element.text\n",
    "                ra_parts = ra.split(\",\")  # Split the string at the comma\n",
    "                if len(ra_parts) > 1:\n",
    "                    ra = ra_parts[0].strip()  # Extract the text before the comma and remove leading/trailing spaces\n",
    "                else:\n",
    "                    ra = ra.strip()  # No comma present, use the whole string and remove leading/trailing spaces\n",
    "            elif \"early action deadline\" in title_text:\n",
    "                ea = value_element.text\n",
    "            elif \"early decision deadline\" in title_text:\n",
    "                ed = value_element.text\n",
    "        except NoSuchElementException:\n",
    "            continue  # Skip this iteration if one of the elements is not found\n",
    "    # Print the extracted information using f-strings\n",
    "    lor_list.append(lor)\n",
    "    wait_list.append(wait)\n",
    "    ra_list.append(ra)\n",
    "    ea_list.append(ea)\n",
    "    ed_list.append(ed)\n",
    "#     print(f\"Letters Required: {lor}\")\n",
    "#     print(f\"Waiting List Used: {wait}\")\n",
    "#     print(f\"Regular Admission Deadline: {ra}\")\n",
    "#     print(f\"Early Action Deadline: {ea}\")\n",
    "#     print(f\"Early Decision Deadline: {ed}\")\n",
    "\n",
    "    # CLICK ON CAMPUS LIFE\n",
    "    print('Clicking on Campus Button.')\n",
    "    campus_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[1]/div[2]/div/div/div/div/a[5]')\n",
    "    campus_button.click()\n",
    "    time.sleep(1)\n",
    "    close_newsletter()     \n",
    "\n",
    "    # CLICK ON LOCATION AND SETTING\n",
    "    print('Clicking on Location and Setting button.')\n",
    "    loc_set_button = driver.find_element_by_xpath('/html/body/div[4]/div/div[1]/div/div[2]/div[1]/div/div[1]/div/div[2]/div')\n",
    "    loc_set_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # UNI AREA SIZE           \n",
    "    print('Getting Uni Area Size.')\n",
    "    # Find all parent elements with the class name \"CollegeProfileContent_content__1hJCl\"\n",
    "    uni_area = 'N/A' #Initialize variable\n",
    "    parent_elements = driver.find_elements_by_class_name(\"CollegeProfileContent_content__1hJCl\")\n",
    "    # Iterate over the parent elements and extract the desired information\n",
    "    for parent_element in parent_elements:\n",
    "        try:\n",
    "            title_element = parent_element.find_element_by_class_name(\"TitleValue_title__2-afK\")\n",
    "            value_element = parent_element.find_element_by_class_name(\"TitleValue_value__1JT0d\")\n",
    "            if title_element.text.lower() == \"campus size\":\n",
    "                uni_area = value_element.text\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            continue  # Skip this iteration if one of the elements is not foundprint(f'Uni Area Size: {uni_area}')\n",
    "#     print(f'Uni Area: {uni_area}')\n",
    "    uni_area_list.append(uni_area)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    if len(uni_name_list) == len(site_lists):\n",
    "        print(f'Completed getting info for all the universities.')\n",
    "    else:\n",
    "        print(f'Completed getting info for {uni_name}. Waiting for next link.')\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac409c4",
   "metadata": {},
   "source": [
    "And we're done!!! That's it. Now we're ready to gather these lists into a dataframe and combine all the information we collected from both websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db280f",
   "metadata": {
    "tags": [
     "cd-pandas"
    ]
   },
   "source": [
    "### CREATE DATAFRAME FROM COLLEGEDATA DATA<a id='cd-pandas'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f3b185c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>Women%</th>\n",
       "      <th>Intl Rate</th>\n",
       "      <th>Countries</th>\n",
       "      <th>RA CD</th>\n",
       "      <th>EA CD</th>\n",
       "      <th>ED CD</th>\n",
       "      <th>Wait List</th>\n",
       "      <th>LOR</th>\n",
       "      <th>Campus Area (acres)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allegheny College</td>\n",
       "      <td>53.9%</td>\n",
       "      <td>4.1%</td>\n",
       "      <td>56</td>\n",
       "      <td>February 15</td>\n",
       "      <td>December 1</td>\n",
       "      <td>November 15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>566 acres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beloit College</td>\n",
       "      <td>52.5%</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>January 15</td>\n",
       "      <td>Not reported</td>\n",
       "      <td>November 1</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>84 acres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     University Name Women% Intl Rate Countries        RA CD         EA CD  \\\n",
       "0  Allegheny College  53.9%      4.1%        56  February 15    December 1   \n",
       "1     Beloit College  52.5%       N/A       N/A   January 15  Not reported   \n",
       "\n",
       "         ED CD Wait List LOR Campus Area (acres)  \n",
       "0  November 15       Yes   2           566 acres  \n",
       "1   November 1        No   1            84 acres  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"University Name\": uni_name_list,\n",
    "    \"Women%\": women_pct_list,\n",
    "    \"Intl Rate\": intl_rate_list,\n",
    "    \"Countries\": intl_ctrs_list,\n",
    "    \"RA CD\" : ra_list,\n",
    "    \"EA CD\" : ea_list,\n",
    "    \"ED CD\" : ed_list,\n",
    "    \"Wait List\": wait_list,\n",
    "    \"LOR\": lor_list,\n",
    "    \"Campus Area (acres)\": uni_area_list\n",
    "    #\"URL\": site_lists\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "collegedata_info = pd.DataFrame(data)\n",
    "\n",
    "collegedata_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b44c95",
   "metadata": {},
   "source": [
    "**Function to fix dates**\n",
    "\n",
    "As mentioned before, some dates have two components, separated by a comma. We want to keep only the first component of these dates, and we want to apply this function only if a comma is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "16967a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_single_date(date):\n",
    "    if ',' in date:\n",
    "        # Split the string at the comma and keep only the first part\n",
    "        date = date.split(',')[0]\n",
    "    \n",
    "    # Remove any leading/trailing spaces\n",
    "    date = date.strip()\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7ccc62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 30\n",
      "January 30\n",
      "N/A\n"
     ]
    }
   ],
   "source": [
    "# Test the extract_single_date function on different date formats to make sure it works.\n",
    "\n",
    "# Sample date values\n",
    "date1 = \"January 30\"\n",
    "date2 = \"January 30, December 15\"\n",
    "date3 = \"N/A\"\n",
    "\n",
    "# Extract single dates\n",
    "date1 = extract_single_date(date1)\n",
    "date2 = extract_single_date(date2)\n",
    "date3 = extract_single_date(date3)\n",
    "\n",
    "print(date1)  # Output: January 30\n",
    "print(date2)  # Output: January 30\n",
    "print(date3)  # Output: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2a7ce",
   "metadata": {},
   "source": [
    "We apply the above function on our 'ED CD' column (early decision column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "261f3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collegedata_info['ED CD'] = collegedata_info['ED CD'].apply(extract_single_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d0186192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>Women%</th>\n",
       "      <th>Intl Rate</th>\n",
       "      <th>Countries</th>\n",
       "      <th>RA CD</th>\n",
       "      <th>EA CD</th>\n",
       "      <th>ED CD</th>\n",
       "      <th>Wait List</th>\n",
       "      <th>LOR</th>\n",
       "      <th>Campus Area (acres)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allegheny College</td>\n",
       "      <td>53.9%</td>\n",
       "      <td>4.1%</td>\n",
       "      <td>56</td>\n",
       "      <td>February 15</td>\n",
       "      <td>December 1</td>\n",
       "      <td>November 15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>566 acres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beloit College</td>\n",
       "      <td>52.5%</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>January 15</td>\n",
       "      <td>Not reported</td>\n",
       "      <td>November 1</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>84 acres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     University Name Women% Intl Rate Countries        RA CD         EA CD  \\\n",
       "0  Allegheny College  53.9%      4.1%        56  February 15    December 1   \n",
       "1     Beloit College  52.5%       N/A       N/A   January 15  Not reported   \n",
       "\n",
       "         ED CD Wait List LOR Campus Area (acres)  \n",
       "0  November 15       Yes   2           566 acres  \n",
       "1   November 1        No   1            84 acres  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collegedata_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f54fc",
   "metadata": {},
   "source": [
    "**SAVE COLLEGEDATA DATA TO CSV**\n",
    "\n",
    "If we are interested only in the CollegeData.com data, we can now save a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "28bb78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "collegedata_info.to_csv('collegedata_uni_info_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ef4ee",
   "metadata": {
    "tags": [
     "merge"
    ]
   },
   "source": [
    "## MERGE BIGFUTURE AND COLLEGEDATA DF'S<a id='merge'></a>\n",
    "\n",
    "\n",
    "The hard part is done! Now we use `pandas` to merge our dataframes and export a final master spreadsheet that combines information from both the websites.\n",
    "\n",
    "First we make sure to import `pandas`, and load our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2186781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List 1 Unis\n",
    "# cd_1 = pd.read_csv('./collegedata_uni_info_1_1.csv')\n",
    "# cd_2 = pd.read_csv('./collegedata_uni_info_1_2.csv')\n",
    "# cd_df = pd.concat([cd_1, cd_2], axis = 0)\n",
    "# bf_df = pd.read_csv('./bigfuture_uni_info_1.csv')\n",
    "\n",
    "\n",
    "# List 2 Unis\n",
    "bf_df = pd.read_csv('./bigfuture_uni_info_2.csv') # csv we had saved earlier after scraping bigfuture site\n",
    "cd_df = pd.read_csv('./collegedata_uni_info_2.csv') # csv we had saved earlier scraping collegedata site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be02cd2",
   "metadata": {},
   "source": [
    "**DOUBLE CHECK UNIVERSITY NAMES / KEY INDEX**\n",
    "\n",
    "We're going to be merging the dataframes based on 'University Name' column as our key index, so we need to be absolutely sure that the University names all match up. It is very likely (and we notice in our example) that some universities may be listed with slight variations in the name, e.g. '&' instead of 'and', 'St' instead of 'Saint', and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "04c46db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>Women%</th>\n",
       "      <th>Intl Rate</th>\n",
       "      <th>Countries</th>\n",
       "      <th>RA CD</th>\n",
       "      <th>EA CD</th>\n",
       "      <th>ED CD</th>\n",
       "      <th>Wait List</th>\n",
       "      <th>LOR</th>\n",
       "      <th>Campus Area (acres)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brigham Young University - Idaho</td>\n",
       "      <td>55.3%</td>\n",
       "      <td>17.4%</td>\n",
       "      <td>116.0</td>\n",
       "      <td>February 15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255 acres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>College of Saint Benedict</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7%</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Rolling</td>\n",
       "      <td>December 15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300 acres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lewis &amp; Clark College</td>\n",
       "      <td>63.6%</td>\n",
       "      <td>4.4%</td>\n",
       "      <td>54.0</td>\n",
       "      <td>January 15</td>\n",
       "      <td>November 1</td>\n",
       "      <td>November 1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137 acres</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     University Name Women% Intl Rate  Countries        RA CD  \\\n",
       "2   Brigham Young University - Idaho  55.3%     17.4%      116.0  February 15   \n",
       "7          College of Saint Benedict    NaN      2.7%        5.0      Rolling   \n",
       "18             Lewis & Clark College  63.6%      4.4%       54.0   January 15   \n",
       "\n",
       "          EA CD       ED CD Wait List  LOR Campus Area (acres)  \n",
       "2           NaN         NaN        No  NaN           255 acres  \n",
       "7   December 15         NaN        No  NaN           300 acres  \n",
       "18   November 1  November 1       Yes  NaN           137 acres  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which rows in the collegedata df is missing in bigfuture df.\n",
    "missing_rows = cd_df[~cd_df['University Name'].isin(bf_df['University Name'])]\n",
    "missing_rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880abe82",
   "metadata": {},
   "source": [
    "We notice that **3 university names** are missing. These are:\n",
    "* Brigham Young University - Idaho\n",
    "* College of Saint Benedict\n",
    "* Lewis & Clark College"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d14f8f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Years</th>\n",
       "      <th>Control</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Setting</th>\n",
       "      <th>SAT Min</th>\n",
       "      <th>SAT Max</th>\n",
       "      <th>Acceptance Rate</th>\n",
       "      <th>Difficulty Level</th>\n",
       "      <th>RA</th>\n",
       "      <th>EA</th>\n",
       "      <th>ED</th>\n",
       "      <th>Net Cost</th>\n",
       "      <th>Tuition</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal Expenses</th>\n",
       "      <th>Avg Need Met</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brigham Young University-Idaho</td>\n",
       "      <td>Rexburg</td>\n",
       "      <td>ID</td>\n",
       "      <td>4</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rural</td>\n",
       "      <td>990.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>96%</td>\n",
       "      <td>Not Selective</td>\n",
       "      <td>Sep 10</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>7038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not available</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>College of St. Benedict</td>\n",
       "      <td>Saint Joseph</td>\n",
       "      <td>MN</td>\n",
       "      <td>4</td>\n",
       "      <td>Private</td>\n",
       "      <td>LAC</td>\n",
       "      <td>Small</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>80%</td>\n",
       "      <td>Less Selective</td>\n",
       "      <td>Jan 15</td>\n",
       "      <td>Dec 15</td>\n",
       "      <td>Not available</td>\n",
       "      <td>28177</td>\n",
       "      <td>52700.0</td>\n",
       "      <td>12160.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>91%</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lewis and Clark Community College</td>\n",
       "      <td>Godfrey</td>\n",
       "      <td>IL</td>\n",
       "      <td>2</td>\n",
       "      <td>Public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Not available</td>\n",
       "      <td>3939</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not available</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      University Name          City State  Years  Control  \\\n",
       "2      Brigham Young University-Idaho       Rexburg    ID      4  Private   \n",
       "7             College of St. Benedict  Saint Joseph    MN      4  Private   \n",
       "18  Lewis and Clark Community College       Godfrey    IL      2   Public   \n",
       "\n",
       "   Type   Size   Setting  SAT Min  SAT Max Acceptance Rate Difficulty Level  \\\n",
       "2   NaN    NaN     Rural    990.0   1200.0             96%    Not Selective   \n",
       "7   LAC  Small  Suburban   1045.0   1235.0             80%   Less Selective   \n",
       "18  NaN    NaN  Suburban      NaN      NaN   Not available              NaN   \n",
       "\n",
       "               RA             EA             ED  Net Cost  Tuition  Housing  \\\n",
       "2          Sep 10  Not available  Not available      7038      NaN      NaN   \n",
       "7          Jan 15         Dec 15  Not available     28177  52700.0  12160.0   \n",
       "18  Not available  Not available  Not available      3939   7500.0      NaN   \n",
       "\n",
       "     Books  Personal Expenses   Avg Need Met  \\\n",
       "2      NaN                NaN  Not available   \n",
       "7   1000.0             1500.0            91%   \n",
       "18     NaN                NaN  Not available   \n",
       "\n",
       "                                                  URL  \n",
       "2   https://bigfuture.collegeboard.org/colleges/br...  \n",
       "7   https://bigfuture.collegeboard.org/colleges/co...  \n",
       "18  https://bigfuture.collegeboard.org/colleges/le...  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which rows in the bigfuture df is missing in collegedata df.\n",
    "missing_rows = bf_df[~bf_df['University Name'].isin(cd_df['University Name'])]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f578033",
   "metadata": {},
   "source": [
    "We notice that **3 university names** are missing. These are:\n",
    "* Brigham Young University-Idaho\n",
    "* College of St. Benedict\n",
    "* Lewis and Clark Community College\n",
    "\n",
    "Well, in this case it's obvious that they are referring to the same universities, they're just written slightly differently. One approach to fix these issues is to replace the names in one of the dataframes with the names from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "dd94b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the university names in the cd_df dataframe\n",
    "cd_df['University Name'].replace({\n",
    "    'Brigham Young University - Idaho': 'Brigham Young University-Idaho',\n",
    "    'College of Saint Benedict': 'College of St. Benedict',\n",
    "    'Lewis & Clark College': 'Lewis and Clark Community College'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55368b",
   "metadata": {},
   "source": [
    "If you now rerun the earlier cells where we checked which uni names are missing in either df, you should get back empty df's, which means that there are no cases where the names are mismatching anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b140c4",
   "metadata": {},
   "source": [
    "**MERGE THE DATAFRAMES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b6c9662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the df's based on the keyindex = 'University Name'\n",
    "merged_df = pd.merge(bf_df, cd_df, on = 'University Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fccb0",
   "metadata": {},
   "source": [
    "**ADDING A TOTAL COST COLUMN**  \n",
    "Notice that we have separate costs such as Tuition, Housing, Books, and Personal Expenses. What if we want to have a column that shows the total cost as well? Well we can do that too. But also realize that some of the values for those fields are missing, which means that if a value in any of the costs is missing, a sum would return NaN.\n",
    "\n",
    "So, this is what we'll do. First we'll sum up Housing costs, Books and Supplies, and Personal Expenses cost, and in case of N/A, we'll simply skip those.\n",
    "\n",
    "Next we'll add at that partial sum to Tuition to get Total Cost. **However**, if Tution value is missing (N/A), our Total Cost will also return N/A, so in this case, we do not skip na.(This made sense because if Tuition (which is the majority portion of the costs) info is missing, we don't want to get a misleading value for Total Costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7c6a3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, calculate the partial sum with skipna=True\n",
    "partial_sum = merged_df[['Housing', 'Books', 'Personal Expenses']].sum(axis = 1, skipna = True)\n",
    "\n",
    "# Next, Calculate the final sum (if Tuition is na, our result will also be na)\n",
    "merged_df['Total Cost'] = partial_sum + merged_df['Tuition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bbc38",
   "metadata": {},
   "source": [
    "**FILLING DEADLINE DATES**\n",
    "\n",
    "Remember that we collected deadlines from BOTH the websites? This was because BigFuture didn't report deadlines for some of the universities, but that information was available in CollegeData. So what we're going to do here is if a deadline ('RA', 'EA', 'ED' columns) has 'Not available' in its field, we'll look at the corresponding deadline columns ('RA CD', 'EA CD', 'ED CD') and get the value from there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "57420634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for the 'RA' column where the value is 'Not available'\n",
    "mask = merged_df['RA'] == 'Not available'\n",
    "\n",
    "# Use boolean indexing to assign the corresponding values from the 'RA CD' column to 'RA'\n",
    "merged_df.loc[mask, 'RA'] = merged_df.loc[mask, 'RA CD'].fillna(merged_df['RA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b0869748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for the 'EA' column where the value is 'Not available'\n",
    "mask = merged_df['EA'] == 'Not available'\n",
    "\n",
    "# Use boolean indexing to assign the corresponding values from the 'EA CD' column to 'EA'\n",
    "merged_df.loc[mask, 'EA'] = merged_df.loc[mask, 'EA CD'].fillna(merged_df['EA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "46136105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for the 'ED' column where the value is 'Not available'\n",
    "mask = merged_df['ED'] == 'Not available'\n",
    "\n",
    "# Use boolean indexing to assign the corresponding values from the 'ED CD' column to 'ED'\n",
    "merged_df.loc[mask, 'ED'] = merged_df.loc[mask, 'ED CD'].fillna(merged_df['ED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941d62c",
   "metadata": {},
   "source": [
    "**REARRANGE AND SELECT COLUMNS**  \n",
    "In this section we'll rearrange and keep specific columns. For instance, we don't need to keep the 'RA CD', 'EA CD', and 'ED CD' columns anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "3d43925e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['University Name', 'City', 'State', 'Years', 'Control', 'Type', 'Size',\n",
       "       'Setting', 'SAT Min', 'SAT Max', 'Acceptance Rate', 'Difficulty Level',\n",
       "       'RA', 'EA', 'ED', 'Net Cost', 'Tuition', 'Housing', 'Books',\n",
       "       'Personal Expenses', 'Avg Need Met', 'URL', 'Women%', 'Intl Rate',\n",
       "       'Countries', 'RA CD', 'EA CD', 'ED CD', 'Wait List', 'LOR',\n",
       "       'Campus Area (acres)', 'Total Cost'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column names\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "05d01789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange and keep specific columns\n",
    "merged_df = merged_df[['University Name', 'Difficulty Level', 'Acceptance Rate',\n",
    "                      'Intl Rate', 'Countries', 'Women%', 'SAT Min', 'SAT Max', 'Avg Need Met',\n",
    "                      'City', 'State', 'Campus Area (acres)', 'Control', 'Type', 'Setting',\n",
    "                       'Total Cost', 'Net Cost', 'Tuition', 'Housing', 'Books', 'Personal Expenses',\n",
    "                       'LOR', 'Wait List', 'Years', 'RA', 'EA', 'ED', 'URL'\n",
    "                      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda38ff6",
   "metadata": {},
   "source": [
    "**FORMAT CURRENCY COLUMNS**\n",
    "\n",
    "The costs column, as it is have int values. We want to format them as currency values with $ sign and a thousands separator. So let's apply that to the different costs columns, while making sure to leave na values as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "139452e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lambda function to format currency values\n",
    "format_currency = lambda x: \"${:,.0f}\".format(x) if pd.notna(x) else \"\"\n",
    "\n",
    "# Apply the formatting function to the 'Amount' column\n",
    "merged_df['Total Cost'] = merged_df['Total Cost'].apply(format_currency)\n",
    "merged_df['Net Cost'] = merged_df['Net Cost'].apply(format_currency)\n",
    "merged_df['Tuition'] = merged_df['Tuition'].apply(format_currency)\n",
    "merged_df['Housing'] = merged_df['Housing'].apply(format_currency)\n",
    "merged_df['Books'] = merged_df['Books'].apply(format_currency)\n",
    "merged_df['Personal Expenses'] = merged_df['Personal Expenses'].apply(format_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe6fdf",
   "metadata": {},
   "source": [
    "**FORMAT CAMPUS SIZE COLUMN**  \n",
    "The Campus Size column shows string values with acres included in the string, e.g. '1,000 acres'. Let's remove the acres part and keep only the numeric value instead!\n",
    "\n",
    "regex demo: https://regex101.com/r/SnLxzj/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "d2894488",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Campus Area (acres)'] = merged_df['Campus Area (acres)'].str.extract(r'(\\d{1,3}(?:,\\d{3})*|\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90aab7",
   "metadata": {},
   "source": [
    "**SORT BY UNIVERSITY NAME**  \n",
    "Last step, let's sort by University Name before saving our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "bc0e95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'University Name'\n",
    "merged_df.sort_values(by = 'University Name', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "4591c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University Name</th>\n",
       "      <th>Difficulty Level</th>\n",
       "      <th>Acceptance Rate</th>\n",
       "      <th>Intl Rate</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Women%</th>\n",
       "      <th>SAT Min</th>\n",
       "      <th>SAT Max</th>\n",
       "      <th>Avg Need Met</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Campus Area (acres)</th>\n",
       "      <th>Control</th>\n",
       "      <th>Type</th>\n",
       "      <th>Setting</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Net Cost</th>\n",
       "      <th>Tuition</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal Expenses</th>\n",
       "      <th>LOR</th>\n",
       "      <th>Wait List</th>\n",
       "      <th>Years</th>\n",
       "      <th>RA</th>\n",
       "      <th>EA</th>\n",
       "      <th>ED</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allegheny College</td>\n",
       "      <td>Less Selective</td>\n",
       "      <td>62%</td>\n",
       "      <td>4.1%</td>\n",
       "      <td>56.0</td>\n",
       "      <td>53.9%</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>90%</td>\n",
       "      <td>Meadville</td>\n",
       "      <td>PA</td>\n",
       "      <td>566</td>\n",
       "      <td>Private</td>\n",
       "      <td>LAC</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>$70,668</td>\n",
       "      <td>$24,227</td>\n",
       "      <td>$54,300</td>\n",
       "      <td>$14,868</td>\n",
       "      <td>$400</td>\n",
       "      <td>$1,100</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>Feb 15</td>\n",
       "      <td>Dec 1</td>\n",
       "      <td>Nov 15</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beloit College</td>\n",
       "      <td>Less Selective</td>\n",
       "      <td>62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.5%</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>96%</td>\n",
       "      <td>Beloit</td>\n",
       "      <td>WI</td>\n",
       "      <td>84</td>\n",
       "      <td>Private</td>\n",
       "      <td>LAC</td>\n",
       "      <td>Urban</td>\n",
       "      <td>$71,446</td>\n",
       "      <td>$24,493</td>\n",
       "      <td>$58,042</td>\n",
       "      <td>$10,740</td>\n",
       "      <td>$1,133</td>\n",
       "      <td>$1,531</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>January 15</td>\n",
       "      <td>Nov 1</td>\n",
       "      <td>Nov 1</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bryn Mawr College</td>\n",
       "      <td>Selective</td>\n",
       "      <td>33%</td>\n",
       "      <td>15.4%</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Bryn Mawr</td>\n",
       "      <td>PA</td>\n",
       "      <td>135</td>\n",
       "      <td>Private</td>\n",
       "      <td>LAC</td>\n",
       "      <td>Suburban</td>\n",
       "      <td></td>\n",
       "      <td>$33,716</td>\n",
       "      <td></td>\n",
       "      <td>$18,690</td>\n",
       "      <td>$1,000</td>\n",
       "      <td>$1,000</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>Jan 15</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Nov 15</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bucknell University</td>\n",
       "      <td>Selective</td>\n",
       "      <td>34%</td>\n",
       "      <td>4.9%</td>\n",
       "      <td>50.0</td>\n",
       "      <td>52.9%</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>0%</td>\n",
       "      <td>Lewisburg</td>\n",
       "      <td>PA</td>\n",
       "      <td>446</td>\n",
       "      <td>Private</td>\n",
       "      <td>Private Uni</td>\n",
       "      <td>Rural</td>\n",
       "      <td>$81,436</td>\n",
       "      <td>$39,493</td>\n",
       "      <td>$64,418</td>\n",
       "      <td>$16,118</td>\n",
       "      <td>$900</td>\n",
       "      <td>$0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>Jan 15</td>\n",
       "      <td>Not available</td>\n",
       "      <td>Nov 15</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Centre College</td>\n",
       "      <td>Less Selective</td>\n",
       "      <td>76%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.9%</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>92%</td>\n",
       "      <td>Danville</td>\n",
       "      <td>KY</td>\n",
       "      <td>160</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>$65,836</td>\n",
       "      <td>$24,226</td>\n",
       "      <td>$50,250</td>\n",
       "      <td>$13,040</td>\n",
       "      <td>$1,200</td>\n",
       "      <td>$1,346</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>Jan 15</td>\n",
       "      <td>December 1</td>\n",
       "      <td>November 15</td>\n",
       "      <td>https://bigfuture.collegeboard.org/colleges/ce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       University Name Difficulty Level Acceptance Rate Intl Rate  Countries  \\\n",
       "0    Allegheny College   Less Selective             62%      4.1%       56.0   \n",
       "1       Beloit College   Less Selective             62%       NaN        NaN   \n",
       "2    Bryn Mawr College        Selective             33%     15.4%       37.0   \n",
       "3  Bucknell University        Selective             34%      4.9%       50.0   \n",
       "4       Centre College   Less Selective             76%       NaN        NaN   \n",
       "\n",
       "  Women%  SAT Min  SAT Max   Avg Need Met       City State  \\\n",
       "0  53.9%   1110.0   1370.0            90%  Meadville    PA   \n",
       "1  52.5%   1230.0   1360.0            96%     Beloit    WI   \n",
       "2    NaN   1300.0   1470.0  Not available  Bryn Mawr    PA   \n",
       "3  52.9%   1180.0   1390.0             0%  Lewisburg    PA   \n",
       "4  51.9%   1150.0   1380.0            92%   Danville    KY   \n",
       "\n",
       "  Campus Area (acres)  Control         Type   Setting Total Cost Net Cost  \\\n",
       "0                 566  Private          LAC  Suburban    $70,668  $24,227   \n",
       "1                  84  Private          LAC     Urban    $71,446  $24,493   \n",
       "2                 135  Private          LAC  Suburban             $33,716   \n",
       "3                 446  Private  Private Uni     Rural    $81,436  $39,493   \n",
       "4                 160  Private          NaN  Suburban    $65,836  $24,226   \n",
       "\n",
       "   Tuition  Housing   Books Personal Expenses LOR Wait List  Years  \\\n",
       "0  $54,300  $14,868    $400            $1,100   2       Yes      4   \n",
       "1  $58,042  $10,740  $1,133            $1,531   1        No      4   \n",
       "2           $18,690  $1,000            $1,000   3       Yes      4   \n",
       "3  $64,418  $16,118    $900                $0   1       Yes      4   \n",
       "4  $50,250  $13,040  $1,200            $1,346   1       Yes      4   \n",
       "\n",
       "           RA             EA           ED  \\\n",
       "0      Feb 15          Dec 1       Nov 15   \n",
       "1  January 15          Nov 1        Nov 1   \n",
       "2      Jan 15  Not available       Nov 15   \n",
       "3      Jan 15  Not available       Nov 15   \n",
       "4      Jan 15     December 1  November 15   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://bigfuture.collegeboard.org/colleges/al...  \n",
       "1  https://bigfuture.collegeboard.org/colleges/be...  \n",
       "2  https://bigfuture.collegeboard.org/colleges/br...  \n",
       "3  https://bigfuture.collegeboard.org/colleges/bu...  \n",
       "4  https://bigfuture.collegeboard.org/colleges/ce...  "
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first 5 rows\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5a183",
   "metadata": {},
   "source": [
    "**DROP DUPLICATE ROWS**  \n",
    "After saving the csv, i noticed that one of the universities had been duplicated multiple times! This is becuase the url for that university was in our list multiple times. So let's drop duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "3eab1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop_duplicates(subset=['University Name', 'Acceptance Rate', 'City'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9f75f",
   "metadata": {},
   "source": [
    "**SAVE OUR MASTER SPREADSHEET**  \n",
    "And we're at the end of the road! What a journey. Time to save this csv and peruse it for our benefit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "81019e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "merged_df.to_csv('uni_list_sheet2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb690b5",
   "metadata": {
    "tags": [
     "end"
    ]
   },
   "source": [
    "## CONCLUSION<a id='end'></a>\n",
    "\n",
    "\n",
    "That's it, we made it. We've **successfully scraped** specific information from two dynamic websites - **BigFuture.org** and **CollegeData.com**. \n",
    "\n",
    "We've used the combined power of `Selenium` and `WebDriver` in `Python` to achieve these results. We've made use of different `find_element` methods depending on the website we were scraping. We've handled exceptions, dealt with pesky popups, and dissected texts/strings to extract only useful information. We've gathered similar data from both sites to ensure that we had the as much of the complete picture as possible. We've used `pandas` to create our dataframes, create and format columns, and finally save our dataframe to a csv.\n",
    "\n",
    "Are we done? For now, yes, at least as far as this notebook is concerned. We had set out to scrape dynamic websites, and we did. But now that we have the data, what kind of information can we glean from it? \n",
    "\n",
    "That is a tale for another day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
